{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncovering Hidden Patterns Through Machine Learning: Lessons from **FizzBuzz** for `MXNet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When data scientist Joel Grus wrote an [article](http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/) on using machine learning to solve the *fizzbuzz* problem last year, most people saw it as an exercise in comedy, perhaps with a warning about the inappropriate use of AI. But we saw a deeper lesson. Certainly, you don’t need AI to solve *fizzbuzz* so long as someone tells you the algorithm underlying the problem. But suppose you discover a seemingly random pattern like fizzbuzz output in nature? Patterns like that exist throughout real life, and no one gives us the algorithm. Machine learning solves such problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This summer, I had an opportunity to interview with an AI -startup that I really liked. And guess what? I was asked to solve *fizzbuzz* using deep learning. Long story short, I didn't get the job offer !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this made me think about why fizzbuzz makes sense as an application of deep learning. On the surface, it’s a silly problem in integer arithmetic (or number theory, if you like to be pedantic). But it generates interesting patterns, and if you saw a list of inputs and outputs without knowing the underlying algorithm, finding a way to predict the outputs would be hard. Therefore, fizzbuzz is an easy way to generate patterns on which you can test deep learning techniques. In this article, we’ll try the popular `MXNet` tools (Amazon.com’s default offering for AWS machine learning) and find that this little exercise takes more effort than one might expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is fizzbuzz ?**\n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Fizz_buzz), fizzbuzz originated as children’s game, but for a long time has been a popular challenge that interviewers give programming candidates. Given an integer x, the programmer has to produce output according to the following rules:\n",
    "\n",
    "Given an integer `x`, the output is determined by the following rules:\n",
    "\n",
    "- if `x` is divisible by 3, output is \"fizz\"\n",
    "- if `x` is divisible by 5, output is \"buzz\"\n",
    "- if `x` is divisible by 15, output is \"fizzbuzz\"\n",
    "- else, the output is `x`\n",
    "\n",
    "A typical output sequence will look like this\n",
    "\n",
    "| Input   |      Output      | \n",
    "|----------|:-------------:|\n",
    "| 1 |  1 |\n",
    "| 2 |  2 |\n",
    "| 3 | \"fizz\" |\n",
    "| 4 | 4 |\n",
    "| 5 | \"buzz\" |\n",
    "| 6 | \"fizz\" |\n",
    "| 7 | 7 |\n",
    "| 8 | 8 |\n",
    "| 9 | \"fizz\" |\n",
    "| 10 | \"buzz\" |\n",
    "| 11 | 11 |\n",
    "| 12 | \"fizz\" |\n",
    "| 13 | 13 |\n",
    "| 14 | 14 |\n",
    "| 15 | \"fizzbuzz\" |\n",
    "| 16 | 16 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The requirements generate a surprisingly complex state machine, so they can reveal whether a novice programming candidate has good organizational skills. But if we know the rules that generate the data, there’s really no need for machine learning. Unfortunately, in real-life, we only have the data. The goal of machine learning is to learn the function that generated the data. In this aspect, *fizzbuzz* provides us with an easy-to-understand dataset and allows us to understand and explore the algorithms.\n",
    "\n",
    "What follows below is a pedantic exercise in understanding how `MXNet` can be used to solve the fizzbuzz problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is `MXNet`?**\n",
    "\n",
    "`MXNet` is a scalable open-source deep learning framework with a [broad base of support](https://www.oreilly.com/ideas/apache-mxnetthe-fruit-of-cross-institutional-collaboration). It scales to multiple GPUs and multiple machines. Amazon.com chose `MXNet` to offer as the deep learning framework in AWS. It is supported by Intel, Dato, Baidu, Microsoft, and MIT among others. We use it in this article because of its popularity and because it is one of the most flexible frameworks in deep learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure of the article**\n",
    "\n",
    "In the subsequent sections, we will do the following\n",
    "\n",
    "1. Structure the problem as a multi-class classification problem\n",
    "2. Generate the *fizzbuzz* data\n",
    "3. Divide the data into train and test\n",
    "4. Build a logistic regression model in `MXNet` from scratch\n",
    "5. Introduce `Gluon`\n",
    "6. Build a multi-layer-perceptron model using `Gluon`\n",
    "\n",
    "The examples are coded in Python because it is compact and well-known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure the problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data, how do we structure this as a machine learning problem? To do supervised machine learning, we need features and a target variable. *Fizzbuzz* can be modeled as a multi-class classification problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target**: The target can be one of the four classes - *fizz*, *buzz*, *fizzbuzz* or *the given number*. The model should predict which of the classes is most likely for an input number. After the four classes are encoded and the model is built, it will return one of four prediction labels. So we will also need a decoder function to convert the label to the corresponding output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input**: Let’s think of a way to do feature engineering for the input. The input is an integer. One option,which Joel Grus employed in his article, is convert the number to its binary representation. The binary representation can be fixed-length and each digit of the fixed-length binary representation can be an input feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example will explain this better.\n",
    "\n",
    "Let’s say, we train the model using the first 1,000 integers. To create a binary representation of fixed length, we first need to find the maximum length of the input vector. All numbers up to 1,000 can be represented in binary within the size 2<sup>10</sup> (which is 1024), so, we need the input vector to be of length 10.\n",
    "\n",
    "The output is encoded in the prediction labels as 0, 1, 2, 3 for “fizzbuzz”, “buzz”, “fizz” and “the given number” respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: 11  \n",
    "Output: 11\n",
    "\n",
    "<img src=\"img/11_new.jpg\" width=\"450\" height=\"450\" />\n",
    "\n",
    "<center> *Figure 1. Input and Output encoding* </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Each digit of the input feature vector will be an input neuron. The output will have 4 neurons, one corresponding to each of the labels.\n",
    "\n",
    "Figure 1 depicts this process.\n",
    "\n",
    "<img src=\"img/fizzbuzz_arch.jpg\" width=\"650\" height=\"350\" />\n",
    "\n",
    "<center> *Figure 2. Training data setup* </center>\n",
    "\n",
    "Now let’s start building models using `MXNet`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing and configuring MXNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the software we need for this example, let’s first import the required Python libraries `numpy` and `mxnet`.\n",
    "\n",
    "One way to install `mxnet` is to run the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#install mxnet. If its already available, upgrade it.\n",
    "!pip install mxnet --upgrade --pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import os\n",
    "mx.random.seed(1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `mxnet`, every array has a context - either on a GPU or CPU. \n",
    "\n",
    "The data size won't be too high for this exercise and a single CPU should suffice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the context to be CPU\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper functions**\n",
    "\n",
    "We need a few helper functions to start with.\n",
    "\n",
    "First, a function to convert the given input integer to its binary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to encode the integer to its binary representation\n",
    "def binary_encode(i, num_digits):\n",
    "    return np.array([i >> d & 1 for d in range(num_digits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a function to encode the target into one of the four classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to encode the target into multi-class\n",
    "def fizz_buzz_encode(i):\n",
    "    if   i % 15 == 0: \n",
    "        return 0\n",
    "    elif i % 5  == 0: \n",
    "        return 1\n",
    "    elif i % 3  == 0: \n",
    "        return 2\n",
    "    else:             \n",
    "        return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is built and we use it to predict it for a given number, we need a function that maps the prediction to the right output. The following function helps us do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given prediction, map it to the correct output label\n",
    "def fizz_buzz(i, prediction):\n",
    "    if prediction == 0:\n",
    "        return \"fizzbuzz\"\n",
    "    elif prediction == 1:\n",
    "        return \"buzz\"\n",
    "    elif prediction == 2:\n",
    "        return \"fizz\"\n",
    "    else:\n",
    "        return str(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create train, validation, test datasets**\n",
    "\n",
    "We now need to create the train, validation and test datasets.\n",
    "\n",
    "Let's generate 100,000 data points. That's the first 100,000 integers.\n",
    "\n",
    "In ideal case, the train, validation and test datasets are randomly selected. But for sake of simplicity, let's use the first 100 integers as test dataset. We won't use this for training. Once we create the model, we will check the accuracy of the model by predicting on the test dataset.\n",
    "\n",
    "We will use the integers 101 to 50,000 as training dataset. We will build the model using this dataset.\n",
    "\n",
    "We will use integers 50,001 to 100,000 as validation dataset. We could use this for hyper-parameter tuning. In this exercise, we wouldn't be doing hyper-parameter tuning though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start by defining the number of integers to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Number of integers to generate\n",
    "MAX_NUMBER = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the input feature vector has to be determined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The input feature vector is determined by NUM_DIGITS\n",
    "NUM_DIGITS = np.log2(MAX_NUMBER).astype(np.int)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train, test and validation datasets are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate training dataset - both features and labels\n",
    "trainX = np.array([binary_encode(i, NUM_DIGITS) for i in range(101, np.int(MAX_NUMBER/2))])\n",
    "trainY = np.array([fizz_buzz_encode(i)          for i in range(101, np.int(MAX_NUMBER/2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate validation dataset - both features and labels\n",
    "valX = np.array([binary_encode(i, NUM_DIGITS) for i in range(np.int(MAX_NUMBER/2), MAX_NUMBER)])\n",
    "valY = np.array([fizz_buzz_encode(i)          for i in range(np.int(MAX_NUMBER/2), MAX_NUMBER)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate test dataset - both features and labels\n",
    "testX = np.array([binary_encode(i, NUM_DIGITS) for i in range(1, 101)])\n",
    "testY = np.array([fizz_buzz_encode(i)          for i in range(1, 101)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created the train, test and validation datasets, let's use `mxnet's` iterator `NDArrayIter`. It allows us to specify batch size for training and also allows us to set a flag to shuffle the data or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the parameters\n",
    "batch_size = 100\n",
    "num_inputs = NUM_DIGITS\n",
    "num_outputs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create iterator for train, test and validation datasets\n",
    "train_data = mx.io.NDArrayIter(trainX, trainY,\n",
    "                               batch_size, shuffle=True)\n",
    "val_data = mx.io.NDArrayIter(valX, valY,\n",
    "                               batch_size, shuffle=True)\n",
    "test_data = mx.io.NDArrayIter(testX, testY,\n",
    "                              batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to write another helper function. This one is to evaluate the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to evaluate accuracy of the model\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    data_iterator.reset()\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return predictions,acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression from scratch**\n",
    "\n",
    "Now that we have the data and the helper functions in place, let's go ahead and build logistic regression from scratch. [This](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter02_supervised-learning/softmax-regression-scratch.ipynb) article does a wonderful job of building a multi-class logistic regression from scratch using `mxnet`. We will follow a similar template for our problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mxnet` comes with a `autograd` package. `autograd` enables automatic differentiation of NDArray operations and is used to compute the gradients of the loss function with respect to the model weights. More about `autograd` can be found [here](https://mxnet.incubator.apache.org/api/python/autograd.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import autograd package\n",
    "from mxnet import autograd, nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to initialize the bias and weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize the weight and bias matrix\n",
    "\n",
    "#weights matrix\n",
    "W = nd.random_normal(shape=(num_inputs, num_outputs))\n",
    "#bias matrix\n",
    "b = nd.random_normal(shape=num_outputs)\n",
    "\n",
    "#Model parameters\n",
    "params = [W, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to allocate space for each parameter's gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the output to be the probability for each of the classes. The sum of the probabilities should sum upto one. This is accomplished by the softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(y_linear):\n",
    "    exp = nd.exp(y_linear-nd.max(y_linear))\n",
    "    norms = nd.sum(exp, axis=0, exclude=True).reshape((-1,1))\n",
    "    return exp / norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function we will be using is softmax cross entropy. Cross-entropy maximizes the log-likelihood given to the correct labels. More about softmax cross-entropy can be read [here](https://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss function\n",
    "def softmax_cross_entropy(yhat, y):\n",
    "    return - nd.nansum(y * nd.log(yhat), axis=0, exclude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    y_linear = nd.dot(X, W) + b\n",
    "    yhat = softmax(y_linear)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model to learn(update) the model parameters(weights and biases), we need to define an optimizer. Stochastic Gradient Descent is one of the most popular methods to update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the optimizer\n",
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets execute the training loops. For each training step, batch_size determines the number of data points that will be passed through the network to learn. Once all data points are passed through the network, one epoch is completed. The parameter `epochs` define the number of times the entire dataset has to be cycled through the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyper paramters for the training\n",
    "epochs = 100\n",
    "learning_rate = .01\n",
    "smoothing_constant = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now execute the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99. Loss: 1.18811465231, Train_acc 0.532825651303, Val_acc 0.5332\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    #at the start of each epoch, the train data iterator is reset\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 4)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0)) \n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "    #the training and validation accuracies are computed\n",
    "    _,val_accuracy = evaluate_accuracy(val_data, net)\n",
    "    _,train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Val_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 100 epochs, the train accuracy and validation accuracy doesn't seem to be great. It is only around 53%. Let's see what's the accuracy on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16'\n",
      " '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29' '30' '31'\n",
      " '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43' '44' '45' '46'\n",
      " '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57' '58' '59' '60' '61'\n",
      " '62' '63' '64' '65' '66' '67' '68' '69' '70' '71' '72' '73' '74' '75' '76'\n",
      " '77' '78' '79' '80' '81' '82' '83' '84' '85' '86' '87' '88' '89' '90' '91'\n",
      " '92' '93' '94' '95' '96' '97' '98' '99' '100']\n",
      "Test Accuracy :  0.53\n"
     ]
    }
   ],
   "source": [
    "#model accuracy on the test dataset\n",
    "predictions,test_accuracy = evaluate_accuracy(test_data, net)\n",
    "output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "print(output)\n",
    "print(\"Test Accuracy : \",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the test dataset is also 53%. The model doesn't seem to be working well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look into doing the same using `Gluon`\n",
    "\n",
    "**What is `Gluon`?**\n",
    "\n",
    "Gluon package is a high-level interface for MXNet. Gluon supports both imperative and symbolic programming, making it easy to train complex models imperatively. More details can be found [here](https://mxnet.incubator.apache.org/api/python/gluon.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-layer perceptron using `Gluon`**\n",
    "\n",
    "Let's build a multi-layer perceptron(MLP) model using `gluon`. \n",
    "\n",
    "MLP is one of the simplest deep learning models. More details can be found on [wikipedia](https://en.wikipedia.org/wiki/Multilayer_perceptron)\n",
    "![](img/MLP.jpg)\n",
    "\n",
    "[image source](http://file.scirp.org/Html/2-2200704_39386.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to import `gluon`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import gluon\n",
    "from mxnet import gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reset the training, test and validation iterators\n",
    "train_data.reset()\n",
    "val_data.reset()\n",
    "test_data.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define the Gluon Sequential Model. Each hidden layer is added sequentially. the `num_hidden` variable defines the number of neurons in each of the hidden layers. The `relu` activation function is used in each of the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define number of neurons in each hidden layer\n",
    "num_hidden = 64\n",
    "#Define the sequential network\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(num_inputs, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous exercise, where we built the model from scratch, the parameters need to be initialized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize parameters\n",
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is defined to be softmax cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define the loss function\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer is chosen to be stochastic gradient descent, similar to the previous model. For this model, we define both the learning rate and momentum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the optimizer\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .02,'momentum':0.9})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define variables/hyper-paramters\n",
    "epochs = 100\n",
    "moving_loss = 0.\n",
    "best_accuracy = 0.\n",
    "best_epoch = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 99. Loss: 0.021456909065, Train_acc 0.99498997996, Val_acc 0.44902\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "for e in range(epochs):\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            cross_entropy = loss(output, label)\n",
    "            cross_entropy.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        if i == 0:\n",
    "            moving_loss = nd.mean(cross_entropy).asscalar()\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * nd.mean(cross_entropy).asscalar()\n",
    "\n",
    "    _,val_accuracy = evaluate_accuracy(val_data, net)\n",
    "    _,train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    \n",
    "    if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            if best_epoch!=-1:\n",
    "                print('deleting previous checkpoint...')\n",
    "                os.remove('mlp-%d.params'%(best_epoch))\n",
    "            best_epoch = e\n",
    "            print('Best validation accuracy found. Checkpointing...')\n",
    "            net.save_params('mlp-%d.params'%(e))\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Val_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy looks good - but validation accuracy is terrible. This generally is a sign of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now predict it on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the parameters\n",
    "net.load_params('mlp-%d.params'%(best_epoch), ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' 'buzz' '6' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13' '14'\n",
      " '15' 'buzz' '17' 'fizz' '19' 'buzz' 'fizz' '22' '23' 'fizz' 'buzz' '26'\n",
      " 'fizz' '28' '29' 'fizzbuzz' '31' 'buzz' 'fizz' '34' 'buzz' 'fizz' '37'\n",
      " '38' '39' 'buzz' '41' 'fizz' '43' 'buzz' '45' 'buzz' '47' 'fizz' '49'\n",
      " 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59' 'fizz' 'buzz'\n",
      " '62' 'fizz' '64' 'buzz' '66' '67' '68' '69' 'buzz' '71' 'fizz' '73' '74'\n",
      " '75' '76' '77' '78' '79' 'buzz' 'fizz' '82' '83' 'fizz' 'buzz' '86' '87'\n",
      " '88' '89' 'fizzbuzz' '91' '92' 'fizz' '94' 'buzz' 'fizz' '97' '98' '99'\n",
      " 'buzz']\n",
      "Test Accuracy :  0.83\n"
     ]
    }
   ],
   "source": [
    "#predict on the test dataset\n",
    "predictions,test_accuracy = evaluate_accuracy(test_data, net)\n",
    "output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "print(output)\n",
    "print(\"Test Accuracy : \",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy is 83%. Let's try to explore another model architecture to see how the model accuracy is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion and Next Steps**\n",
    "\n",
    "This article gave a quick overview of how to get started with `mxnet` and `gluon`. With `gluon`, prototyping and experimenting with various architectures is much faster. `autograd` allows us to record computation history on the fly so as to calculate gradients later. \n",
    "\n",
    "[The straight dope](https://github.com/zackchase/mxnet-the-straight-dope/) has a good set of tutorials to get started with `mxnet`.\n",
    "\n",
    "While this article briefly touched upon Convolutional Neural Networks, it didn't provide enough motivation and real-life use cases. While we saw models overfitting, we didn't get into how to overcome overfitting. In the next article, we will show how to build models to do image recognition with some real-life use cases.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
