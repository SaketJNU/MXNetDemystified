{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grokking **FizzBuzz** using `MXNet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last year, Joel Grus wrote a brilliant [article](http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/) on *fizzbuzz*. He had attended an interview and was asked about *fizzbuzz*. He went on to solve it using `tensorflow`. Now, you may wonder why should we use deep learning for this? Atleast, I thought that way and didn't really pay much attention to the code. \n",
    "\n",
    "This summer, I had an opportunity to interview with a AI-startup that I really liked. And guess what? I was asked to solve `fizzbuzz` using deep learning. Long story short, neither Joel nor I got the job ! \n",
    "\n",
    "But this made me think about why `fizzbuzz` makes sense? But before we get on to that, what is this `fizzbuzz` problem?\n",
    "\n",
    "**What is fizzbuzz**\n",
    "\n",
    "Given an integer `x`, the output is determined by the following rules:\n",
    "\n",
    "- if `x` is divisible by 3, output is \"fizz\"\n",
    "- if `x` is divisible by 5, output is \"buzz\"\n",
    "- if `x` is divisible by 15, output is \"fizzbuzz\"\n",
    "- else, the output is `x`\n",
    "\n",
    "A typical output sequence will look like this\n",
    "\n",
    "| Input   |      Output      | \n",
    "|----------|:-------------:|\n",
    "| 1 |  1 |\n",
    "| 2 |  2 |\n",
    "| 3 | \"fizz\" |\n",
    "| 4 | 4 |\n",
    "| 5 | \"buzz\" |\n",
    "| 6 | \"fizz\" |\n",
    "| 7 | 7 |\n",
    "| 8 | 8 |\n",
    "| 9 | \"fizz\" |\n",
    "| 10 | \"buzz\" |\n",
    "| 11 | 11 |\n",
    "| 12 | \"fizz\" |\n",
    "| 13 | 13 |\n",
    "| 14 | 14 |\n",
    "| 15 | \"fizzbuzz\" |\n",
    "| 16 | 16 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know the rules that generate the data, there's really no need for machine learning. Unfortunately, in real-life, we only have the data. The goal of machine learning is to learn the function that generated the data. In this aspect, `fizzbuzz` provides us with an easy-to-understand dataset and allows us to understand the algorithms better. \n",
    "\n",
    "What follows below is a pedantic exercise in understanding how `MXNet` can be used to solve the `fizzbuzz` problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is `MXNet`**\n",
    "\n",
    "`MXNet` is a scalable open-source deep learning framework. It scales to multiple GPUs and multiple machines. At Amazon, `MXNet` is the deep learning framework of choice at AWS. It is supported by Intel, Dato, Baidu, Microsoft, MIT amongst others. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's the structure of the article?**\n",
    "\n",
    "In the subsequent sections, we will do the following\n",
    "\n",
    "1. Generate the fizzbuzz data\n",
    "2. Divide the data into train and test\n",
    "3. Structure the problem as a multi-class classification problem\n",
    "4. Build a logistic regression in `MXNet` from scratch\n",
    "5. Build logistic regression using `MXNet`\n",
    "6. Introduce `Gluon`\n",
    "7. Build a multi-layer-perceptron using `Gluon`\n",
    "8. Build a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import nd\n",
    "import os\n",
    "mx.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a function to encode the integer to its binary representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_encode(i, num_digits):\n",
    "    return np.array([i >> d & 1 for d in range(num_digits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a function to label the data and map the labels back to categorical strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fizz_buzz_encode(i):\n",
    "    if   i % 15 == 0: \n",
    "        return 0\n",
    "    elif i % 5  == 0: \n",
    "        return 1\n",
    "    elif i % 3  == 0: \n",
    "        return 2\n",
    "    else:             \n",
    "        return 3\n",
    "    \n",
    "def fizz_buzz(i, prediction):\n",
    "    if prediction == 0:\n",
    "        return \"fizzbuzz\"\n",
    "    elif prediction == 1:\n",
    "        return \"buzz\"\n",
    "    elif prediction == 2:\n",
    "        return \"fizz\"\n",
    "    else:\n",
    "        return str(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the Numpy NdArray for training, validation and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NUMBER = 100000\n",
    "NUM_DIGITS = np.log2(MAX_NUMBER).astype(np.int)+1\n",
    "trainX = np.array([binary_encode(i, NUM_DIGITS) for i in range(101, np.int(MAX_NUMBER/2))])\n",
    "trainY = np.array([fizz_buzz_encode(i)          for i in range(101, np.int(MAX_NUMBER/2))])\n",
    "valX = np.array([binary_encode(i, NUM_DIGITS) for i in range(np.int(MAX_NUMBER/2), MAX_NUMBER)])\n",
    "valY = np.array([fizz_buzz_encode(i)          for i in range(np.int(MAX_NUMBER/2), MAX_NUMBER)])\n",
    "testX = np.array([binary_encode(i, NUM_DIGITS) for i in range(1, 101)])\n",
    "testY = np.array([fizz_buzz_encode(i)          for i in range(1, 101)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create mxnet NDarrayiter for training, validation and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_inputs = NUM_DIGITS\n",
    "num_outputs = 4\n",
    "train_data = mx.io.NDArrayIter(trainX, trainY,\n",
    "                               batch_size, shuffle=True)\n",
    "val_data = mx.io.NDArrayIter(valX, valY,\n",
    "                               batch_size, shuffle=True)\n",
    "test_data = mx.io.NDArrayIter(testX, testY,\n",
    "                              batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets define the function to calculate accuracy of a model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    data_iterator.reset()\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return predictions,acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the bias and weight matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_scale = .01\n",
    "\n",
    "W = nd.random_normal(shape=(num_inputs, num_outputs))\n",
    "b = nd.random_normal(shape=num_outputs)\n",
    "\n",
    "params = [W, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Allocate space for each parameter's gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall pass our $yhat\\_linear$ and compute the softmax and its log all at once inside the $softmax\\_cross\\_entropy$ loss function simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_cross_entropy(yhat_linear, y):\n",
    "    return - nd.nansum(y * nd.log_softmax(yhat_linear), axis=0, exclude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    y_linear = nd.dot(X, W) + b\n",
    "    return y_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets execute the training loops**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 1.2058725952, Train_acc 0.532805611222, Val_acc 0.53322\n",
      "Epoch 1. Loss: 1.18823441638, Train_acc 0.532825651303, Val_acc 0.53318\n",
      "Epoch 2. Loss: 1.18811540162, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 3. Loss: 1.18811462552, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 4. Loss: 1.18811465773, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 5. Loss: 1.18811464431, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 6. Loss: 1.1881146574, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 7. Loss: 1.18811464841, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 8. Loss: 1.18811465812, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 9. Loss: 1.18811466016, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 10. Loss: 1.18811465319, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 11. Loss: 1.18811465092, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 12. Loss: 1.18811465107, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 13. Loss: 1.18811465962, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 14. Loss: 1.18811464822, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 15. Loss: 1.18811465759, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 16. Loss: 1.18811465602, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 17. Loss: 1.18811466476, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 18. Loss: 1.18811464741, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 19. Loss: 1.1881146599, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 20. Loss: 1.18811465328, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 21. Loss: 1.18811465257, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 22. Loss: 1.18811466643, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 23. Loss: 1.18811465471, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 24. Loss: 1.18811465534, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 25. Loss: 1.18811465139, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 26. Loss: 1.18811465107, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 27. Loss: 1.18811466067, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 28. Loss: 1.18811466137, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 29. Loss: 1.18811466371, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 30. Loss: 1.18811465122, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 31. Loss: 1.18811465418, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 32. Loss: 1.18811465303, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 33. Loss: 1.18811466248, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 34. Loss: 1.18811465798, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 35. Loss: 1.18811465756, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 36. Loss: 1.18811465391, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 37. Loss: 1.18811465732, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 38. Loss: 1.18811465715, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 39. Loss: 1.18811464713, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 40. Loss: 1.1881146493, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 41. Loss: 1.18811465312, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 42. Loss: 1.18811466108, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 43. Loss: 1.18811464855, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 44. Loss: 1.18811465257, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 45. Loss: 1.1881146498, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 46. Loss: 1.18811464865, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 47. Loss: 1.18811465167, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 48. Loss: 1.18811465738, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 49. Loss: 1.18811466396, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 50. Loss: 1.18811465078, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 51. Loss: 1.1881146438, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 52. Loss: 1.18811465064, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 53. Loss: 1.18811466092, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 54. Loss: 1.18811465122, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 55. Loss: 1.18811466185, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 56. Loss: 1.18811465338, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 57. Loss: 1.18811466276, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 58. Loss: 1.18811465642, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 59. Loss: 1.18811465463, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 60. Loss: 1.18811465991, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 61. Loss: 1.18811466145, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 62. Loss: 1.18811465096, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 63. Loss: 1.18811466298, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 64. Loss: 1.18811464998, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 65. Loss: 1.18811465641, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 66. Loss: 1.18811466234, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 67. Loss: 1.18811466033, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 68. Loss: 1.18811465301, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 69. Loss: 1.18811465269, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 70. Loss: 1.18811464648, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 71. Loss: 1.18811465278, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 72. Loss: 1.18811465039, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 73. Loss: 1.18811465566, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 74. Loss: 1.1881146561, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 75. Loss: 1.18811466923, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 76. Loss: 1.18811464609, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 77. Loss: 1.1881146572, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 78. Loss: 1.18811465586, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 79. Loss: 1.18811466441, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 80. Loss: 1.18811465788, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 81. Loss: 1.18811465275, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 82. Loss: 1.1881146465, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 83. Loss: 1.18811466353, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 84. Loss: 1.18811465094, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 85. Loss: 1.18811465732, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 86. Loss: 1.18811464705, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 87. Loss: 1.1881146515, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 88. Loss: 1.18811464568, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 89. Loss: 1.18811465958, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 90. Loss: 1.18811465609, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 91. Loss: 1.18811465696, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 92. Loss: 1.1881146469, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 93. Loss: 1.18811465069, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 94. Loss: 1.18811465448, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 95. Loss: 1.18811466161, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 96. Loss: 1.18811465158, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 97. Loss: 1.18811465283, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 98. Loss: 1.18811465057, Train_acc 0.532825651303, Val_acc 0.5332\n",
      "Epoch 99. Loss: 1.18811465231, Train_acc 0.532825651303, Val_acc 0.5332\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "learning_rate = .01\n",
    "smoothing_constant = .01\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 4)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0)) \n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "    _,val_accuracy = evaluate_accuracy(val_data, net)\n",
    "    _,train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Val_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets see what the model predicts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16'\n",
      " '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29' '30' '31'\n",
      " '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43' '44' '45' '46'\n",
      " '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57' '58' '59' '60' '61'\n",
      " '62' '63' '64' '65' '66' '67' '68' '69' '70' '71' '72' '73' '74' '75' '76'\n",
      " '77' '78' '79' '80' '81' '82' '83' '84' '85' '86' '87' '88' '89' '90' '91'\n",
      " '92' '93' '94' '95' '96' '97' '98' '99' '100']\n",
      "Test Accuracy :  0.53\n"
     ]
    }
   ],
   "source": [
    "predictions,test_accuracy = evaluate_accuracy(test_data, net)\n",
    "output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "print(output)\n",
    "print(\"Test Accuracy : \",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiLayer Perceptron using Gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets reset the Training, Validation and the Test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.reset()\n",
    "val_data.reset()\n",
    "test_data.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Gluon Sequential Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden = 64\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(num_inputs, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize Parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax Cross Entropy Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .02,'momentum':0.9})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets Train the MLP model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 0. Loss: 1.14648244872, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 1. Loss: 1.14491358918, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 2. Loss: 1.14468319246, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 3. Loss: 1.14453604449, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 4. Loss: 1.14444865325, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 5. Loss: 1.14438332629, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 6. Loss: 1.14432253718, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 7. Loss: 1.14426912234, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 8. Loss: 1.14421235097, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 9. Loss: 1.14415274821, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 10. Loss: 1.14404541124, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 11. Loss: 1.14379836367, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 12. Loss: 1.14291108788, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 13. Loss: 1.13767488342, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 14. Loss: 1.10466560823, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 15. Loss: 1.00281759353, Train_acc 0.542344689379, Val_acc 0.52126\n",
      "Epoch 16. Loss: 0.917636309477, Train_acc 0.548937875752, Val_acc 0.38344\n",
      "Epoch 17. Loss: 0.85348202285, Train_acc 0.553527054108, Val_acc 0.40204\n",
      "Epoch 18. Loss: 0.706650876827, Train_acc 0.648196392786, Val_acc 0.3889\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 19. Loss: 0.524540588308, Train_acc 0.804088176353, Val_acc 0.55008\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 20. Loss: 0.448689267491, Train_acc 0.811062124248, Val_acc 0.56044\n",
      "Epoch 21. Loss: 0.362474109912, Train_acc 0.862384769539, Val_acc 0.46344\n",
      "Epoch 22. Loss: 0.254539941044, Train_acc 0.919038076152, Val_acc 0.4784\n",
      "Epoch 23. Loss: 0.207288567778, Train_acc 0.954148296593, Val_acc 0.434\n",
      "Epoch 24. Loss: 0.26223864075, Train_acc 0.958276553106, Val_acc 0.42488\n",
      "Epoch 25. Loss: 0.152736466378, Train_acc 0.959899799599, Val_acc 0.41934\n",
      "Epoch 26. Loss: 0.142343568941, Train_acc 0.95380761523, Val_acc 0.42582\n",
      "Epoch 27. Loss: 0.1011174189, Train_acc 0.962845691383, Val_acc 0.41282\n",
      "Epoch 28. Loss: 0.347877876729, Train_acc 0.938537074148, Val_acc 0.4263\n",
      "Epoch 29. Loss: 0.0754076581199, Train_acc 0.946753507014, Val_acc 0.40098\n",
      "Epoch 30. Loss: 0.0612130074886, Train_acc 0.98130260521, Val_acc 0.43006\n",
      "Epoch 31. Loss: 0.0682303481632, Train_acc 0.969639278557, Val_acc 0.42856\n",
      "Epoch 32. Loss: 0.0735422400118, Train_acc 0.970641282565, Val_acc 0.4229\n",
      "Epoch 33. Loss: 0.406197116969, Train_acc 0.928296593186, Val_acc 0.41168\n",
      "Epoch 34. Loss: 0.1235438064, Train_acc 0.979138276553, Val_acc 0.44286\n",
      "Epoch 35. Loss: 0.0951230467892, Train_acc 0.98496993988, Val_acc 0.4375\n",
      "Epoch 36. Loss: 0.0914621994127, Train_acc 0.981703406814, Val_acc 0.4373\n",
      "Epoch 37. Loss: 0.0775746683342, Train_acc 0.98250501002, Val_acc 0.44432\n",
      "Epoch 38. Loss: 0.0621128956114, Train_acc 0.984809619238, Val_acc 0.44274\n",
      "Epoch 39. Loss: 0.0557778986875, Train_acc 0.987334669339, Val_acc 0.44244\n",
      "Epoch 40. Loss: 0.0490225325505, Train_acc 0.987034068136, Val_acc 0.43336\n",
      "Epoch 41. Loss: 0.049397254897, Train_acc 0.988837675351, Val_acc 0.43852\n",
      "Epoch 42. Loss: 0.0415048960686, Train_acc 0.990821643287, Val_acc 0.44374\n",
      "Epoch 43. Loss: 0.0392982401743, Train_acc 0.990861723447, Val_acc 0.44204\n",
      "Epoch 44. Loss: 0.162397180708, Train_acc 0.983507014028, Val_acc 0.44618\n",
      "Epoch 45. Loss: 0.0459975678164, Train_acc 0.98877755511, Val_acc 0.44\n",
      "Epoch 46. Loss: 0.0356117998728, Train_acc 0.98877755511, Val_acc 0.44154\n",
      "Epoch 47. Loss: 0.0381584048225, Train_acc 0.989358717435, Val_acc 0.44106\n",
      "Epoch 48. Loss: 0.0354685856798, Train_acc 0.988476953908, Val_acc 0.44256\n",
      "Epoch 49. Loss: 0.0367434292617, Train_acc 0.986513026052, Val_acc 0.44748\n",
      "Epoch 50. Loss: 0.0345272973393, Train_acc 0.988897795591, Val_acc 0.44034\n",
      "Epoch 51. Loss: 0.0473305580558, Train_acc 0.964649298597, Val_acc 0.42172\n",
      "Epoch 52. Loss: 0.0719253603154, Train_acc 0.988336673347, Val_acc 0.43734\n",
      "Epoch 53. Loss: 0.0579176852759, Train_acc 0.988857715431, Val_acc 0.44068\n",
      "Epoch 54. Loss: 0.0733668314026, Train_acc 0.98879759519, Val_acc 0.43994\n",
      "Epoch 55. Loss: 0.0552044600043, Train_acc 0.987855711423, Val_acc 0.43776\n",
      "Epoch 56. Loss: 0.055466870499, Train_acc 0.989218436874, Val_acc 0.43194\n",
      "Epoch 57. Loss: 0.046102081253, Train_acc 0.987334669339, Val_acc 0.44026\n",
      "Epoch 58. Loss: 0.234130828595, Train_acc 0.774088176353, Val_acc 0.40392\n",
      "Epoch 59. Loss: 0.0700240399025, Train_acc 0.986893787575, Val_acc 0.44856\n",
      "Epoch 60. Loss: 0.0504152480586, Train_acc 0.991102204409, Val_acc 0.4467\n",
      "Epoch 61. Loss: 0.0436607185901, Train_acc 0.986673346693, Val_acc 0.43686\n",
      "Epoch 62. Loss: 0.0361764202248, Train_acc 0.990380761523, Val_acc 0.43856\n",
      "Epoch 63. Loss: 0.034623035384, Train_acc 0.991643286573, Val_acc 0.43902\n",
      "Epoch 64. Loss: 0.0307580780831, Train_acc 0.991603206413, Val_acc 0.4437\n",
      "Epoch 65. Loss: 0.0306939773913, Train_acc 0.988637274549, Val_acc 0.43646\n",
      "Epoch 66. Loss: 0.0293248269347, Train_acc 0.99250501002, Val_acc 0.44008\n",
      "Epoch 67. Loss: 0.0275316861626, Train_acc 0.99376753507, Val_acc 0.44346\n",
      "Epoch 68. Loss: 0.0246706183806, Train_acc 0.993206412826, Val_acc 0.44162\n",
      "Epoch 69. Loss: 0.0272334974098, Train_acc 0.994468937876, Val_acc 0.44114\n",
      "Epoch 70. Loss: 0.0247537878235, Train_acc 0.991022044088, Val_acc 0.44042\n",
      "Epoch 71. Loss: 0.0224526091835, Train_acc 0.994228456914, Val_acc 0.44492\n",
      "Epoch 72. Loss: 0.0216411354028, Train_acc 0.994529058116, Val_acc 0.44808\n",
      "Epoch 73. Loss: 0.023013151418, Train_acc 0.994328657315, Val_acc 0.45302\n",
      "Epoch 74. Loss: 0.154429048625, Train_acc 0.95751503006, Val_acc 0.41792\n",
      "Epoch 75. Loss: 0.0506291146806, Train_acc 0.993907815631, Val_acc 0.4432\n",
      "Epoch 76. Loss: 0.0389491060604, Train_acc 0.993366733467, Val_acc 0.44924\n",
      "Epoch 77. Loss: 0.0279218228158, Train_acc 0.991923847695, Val_acc 0.44564\n",
      "Epoch 78. Loss: 0.0227274039928, Train_acc 0.995851703407, Val_acc 0.44656\n",
      "Epoch 79. Loss: 0.0216108003047, Train_acc 0.993647294589, Val_acc 0.44938\n",
      "Epoch 80. Loss: 0.0213814747905, Train_acc 0.994228456914, Val_acc 0.4492\n",
      "Epoch 81. Loss: 0.0234862877983, Train_acc 0.992665330661, Val_acc 0.44282\n",
      "Epoch 82. Loss: 0.0190997374497, Train_acc 0.994348697395, Val_acc 0.4497\n",
      "Epoch 83. Loss: 0.0194089538705, Train_acc 0.994288577154, Val_acc 0.44836\n",
      "Epoch 84. Loss: 0.0176207793402, Train_acc 0.995651302605, Val_acc 0.45142\n",
      "Epoch 85. Loss: 0.01717413718, Train_acc 0.99501002004, Val_acc 0.45498\n",
      "Epoch 86. Loss: 0.017733375502, Train_acc 0.995370741483, Val_acc 0.45488\n",
      "Epoch 87. Loss: 0.0242469017096, Train_acc 0.994629258517, Val_acc 0.45388\n",
      "Epoch 88. Loss: 0.0175423781748, Train_acc 0.99503006012, Val_acc 0.45012\n",
      "Epoch 89. Loss: 0.426297970603, Train_acc 0.833126252505, Val_acc 0.40282\n",
      "Epoch 90. Loss: 0.0587983846646, Train_acc 0.988937875752, Val_acc 0.45184\n",
      "Epoch 91. Loss: 0.0317582800702, Train_acc 0.989619238477, Val_acc 0.44062\n",
      "Epoch 92. Loss: 0.0281071907707, Train_acc 0.989298597194, Val_acc 0.43926\n",
      "Epoch 93. Loss: 0.0264104260341, Train_acc 0.990761523046, Val_acc 0.44328\n",
      "Epoch 94. Loss: 0.0248388817964, Train_acc 0.992625250501, Val_acc 0.4468\n",
      "Epoch 95. Loss: 0.0242339994857, Train_acc 0.99250501002, Val_acc 0.44728\n",
      "Epoch 96. Loss: 0.0235712779235, Train_acc 0.99124248497, Val_acc 0.44382\n",
      "Epoch 97. Loss: 0.0217393858143, Train_acc 0.993527054108, Val_acc 0.44606\n",
      "Epoch 98. Loss: 0.0216870269234, Train_acc 0.993867735471, Val_acc 0.44714\n",
      "Epoch 99. Loss: 0.021456909065, Train_acc 0.99498997996, Val_acc 0.44902\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "moving_loss = 0.\n",
    "best_accuracy = 0.\n",
    "best_epoch = -1\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            cross_entropy = loss(output, label)\n",
    "            cross_entropy.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        if i == 0:\n",
    "            moving_loss = nd.mean(cross_entropy).asscalar()\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * nd.mean(cross_entropy).asscalar()\n",
    "\n",
    "    _,val_accuracy = evaluate_accuracy(val_data, net)\n",
    "    _,train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    \n",
    "    if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            if best_epoch!=-1:\n",
    "                print('deleting previous checkpoint...')\n",
    "                os.remove('mlp-%d.params'%(best_epoch))\n",
    "            best_epoch = e\n",
    "            print('Best validation accuracy found. Checkpointing...')\n",
    "            net.save_params('mlp-%d.params'%(e))\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Val_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets see what the model predicts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.load_params('mlp-%d.params'%(best_epoch), ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' 'buzz' '6' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13' '14'\n",
      " '15' 'buzz' '17' 'fizz' '19' 'buzz' 'fizz' '22' '23' 'fizz' 'buzz' '26'\n",
      " 'fizz' '28' '29' 'fizzbuzz' '31' 'buzz' 'fizz' '34' 'buzz' 'fizz' '37'\n",
      " '38' '39' 'buzz' '41' 'fizz' '43' 'buzz' '45' 'buzz' '47' 'fizz' '49'\n",
      " 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59' 'fizz' 'buzz'\n",
      " '62' 'fizz' '64' 'buzz' '66' '67' '68' '69' 'buzz' '71' 'fizz' '73' '74'\n",
      " '75' '76' '77' '78' '79' 'buzz' 'fizz' '82' '83' 'fizz' 'buzz' '86' '87'\n",
      " '88' '89' 'fizzbuzz' '91' '92' 'fizz' '94' 'buzz' 'fizz' '97' '98' '99'\n",
      " 'buzz']\n",
      "Test Accuracy :  0.83\n"
     ]
    }
   ],
   "source": [
    "predictions,test_accuracy = evaluate_accuracy(test_data, net)\n",
    "output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "print(output)\n",
    "print(\"Test Accuracy : \",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN using mxnet symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets reshape the data (x_dim,y_dim) &rarr; (x_dim,#of channels = 1,y_dim)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX= trainX.reshape(trainX.shape[0],1,trainX.shape[1])\n",
    "valX= valX.reshape(valX.shape[0],1,valX.shape[1])\n",
    "testX= testX.reshape(testX.shape[0],1,testX.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare the NDArrayIters corresponding to Training, Testing and Validation data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = mx.io.NDArrayIter(trainX, trainY,\n",
    "                               batch_size, shuffle=True)\n",
    "val_data = mx.io.NDArrayIter(valX, valY,\n",
    "                               batch_size, shuffle=True)\n",
    "test_data = mx.io.NDArrayIter(testX, testY,\n",
    "                              batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = mx.sym.var('data')\n",
    "# first conv layer\n",
    "conv1 = mx.sym.Convolution(data=data, kernel=(2,), num_filter=20)\n",
    "tanh1 = mx.sym.Activation(data=conv1, act_type=\"relu\")\n",
    "pool1 = mx.sym.Pooling(data=tanh1, pool_type=\"max\", kernel=(2,), stride=(2,))\n",
    "# second conv layer\n",
    "conv2 = mx.sym.Convolution(data=pool1, kernel=(2,), num_filter=50)\n",
    "tanh2 = mx.sym.Activation(data=conv2, act_type=\"relu\")\n",
    "pool2 = mx.sym.Pooling(data=tanh2, pool_type=\"max\", kernel=(2,), stride=(2,))\n",
    "# first fullc layer\n",
    "flatten = mx.sym.flatten(data=pool2)\n",
    "fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)\n",
    "tanh3 = mx.sym.Activation(data=fc1, act_type=\"relu\")\n",
    "# second fullc\n",
    "fc2 = mx.sym.FullyConnected(data=tanh3, num_hidden=num_outputs)\n",
    "# softmax loss\n",
    "lenet = mx.sym.SoftmaxOutput(data=fc2, name='softmax')\n",
    "cnn_model = mx.mod.Module(symbol=lenet, context=ctx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_model.fit(train_data,\n",
    "                eval_data=val_data,\n",
    "                optimizer='sgd',\n",
    "                optimizer_params={'learning_rate':0.01,'momentum':0.9},\n",
    "                eval_metric='acc',\n",
    "                num_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets see what the model predicts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16'\n",
      " '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29' '30' '31'\n",
      " '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43' '44' '45' '46'\n",
      " '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57' '58' '59' '60' '61'\n",
      " '62' '63' '64' '65' '66' '67' '68' '69' '70' '71' '72' '73' '74' '75' '76'\n",
      " '77' '78' '79' '80' '81' '82' '83' '84' '85' '86' '87' '88' '89' '90' '91'\n",
      " '92' '93' '94' '95' '96' '97' '98' '99' '100']\n",
      "Test Accuracy :  0.53\n"
     ]
    }
   ],
   "source": [
    "acc = mx.metric.Accuracy()\n",
    "cnn_model.score(test_data, acc)\n",
    "probabilities = cnn_model.predict(test_data)\n",
    "predictions = nd.argmax(probabilities, axis=1)\n",
    "output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "print(output)\n",
    "print(\"Test Accuracy : \",acc.get_name_value()[0][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
