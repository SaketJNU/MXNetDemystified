{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grokking **FizzBuzz** using `MXNet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last year, Joel Grus wrote a brilliant [article](http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/) on *fizzbuzz*. He had attended an interview and was asked about *fizzbuzz*. He went on to solve it using `tensorflow`. Now, you may wonder why should we use deep learning for this? Atleast, I thought that way and didn't really pay much attention to the code. \n",
    "\n",
    "This summer, I had an opportunity to interview with an AI-startup that I really liked. And guess what? I was asked to solve `fizzbuzz` using deep learning. Long story short, neither Joel nor I got the job ! \n",
    "\n",
    "But this made me think about why `fizzbuzz` makes sense? But before we get on to that, what is this `fizzbuzz` problem?\n",
    "\n",
    "**What is fizzbuzz ?**\n",
    "\n",
    "Given an integer `x`, the output is determined by the following rules:\n",
    "\n",
    "- if `x` is divisible by 3, output is \"fizz\"\n",
    "- if `x` is divisible by 5, output is \"buzz\"\n",
    "- if `x` is divisible by 15, output is \"fizzbuzz\"\n",
    "- else, the output is `x`\n",
    "\n",
    "A typical output sequence will look like this\n",
    "\n",
    "| Input   |      Output      | \n",
    "|----------|:-------------:|\n",
    "| 1 |  1 |\n",
    "| 2 |  2 |\n",
    "| 3 | \"fizz\" |\n",
    "| 4 | 4 |\n",
    "| 5 | \"buzz\" |\n",
    "| 6 | \"fizz\" |\n",
    "| 7 | 7 |\n",
    "| 8 | 8 |\n",
    "| 9 | \"fizz\" |\n",
    "| 10 | \"buzz\" |\n",
    "| 11 | 11 |\n",
    "| 12 | \"fizz\" |\n",
    "| 13 | 13 |\n",
    "| 14 | 14 |\n",
    "| 15 | \"fizzbuzz\" |\n",
    "| 16 | 16 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know the rules that generate the data, there's really no need for machine learning. Unfortunately, in real-life, we only have the data. The goal of machine learning is to learn the function that generated the data. In this aspect, `fizzbuzz` provides us with an easy-to-understand dataset and allows us to understand and explore the algorithms better. \n",
    "\n",
    "What follows below is a pedantic exercise in understanding how `MXNet` can be used to solve the `fizzbuzz` problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is `MXNet`?**\n",
    "\n",
    "`MXNet` is a scalable open-source deep learning framework. It scales to multiple GPUs and multiple machines. At Amazon, `MXNet` is the deep learning framework of choice at AWS. It is supported by Intel, Dato, Baidu, Microsoft, MIT amongst others. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure of the article**\n",
    "\n",
    "In the subsequent sections, we will do the following\n",
    "\n",
    "1. Structure the problem as a multi-class classification problem\n",
    "2. Generate the fizzbuzz data\n",
    "3. Divide the data into train and test\n",
    "4. Build a logistic regression model in `MXNet` from scratch\n",
    "5. Introduce `Gluon`\n",
    "6. Build a multi-layer-perceptron model using `Gluon`\n",
    "7. Build a Convolutional Neural Network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure the problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data, how do we structure this as a machine learning problem? To do supervised machine learning, we need features and a target variable. *Fizzbuzz* can be modeled as a multi-class classification problem. \n",
    "\n",
    "Let's first start with the target. The target can be one of the four classes - \"fizz\", \"buzz\", \"fizzbuzz\" or \"the given number\". The model should predict which of the classes is most likely for the given input number. The 4 classes are encoded and the model will be built. Once the model is built, using the prediction label, we will need a decoder function. The decoder function will convert the label to the corresponding output. \n",
    "\n",
    "Now, let's think of a way to do feature engineering for the input. The input is an integer. One option we could explore is convert the number to its binary representation. The binary representation could be of fixed-length. And each digit of the fixed-length binary representation can be an input feature.\n",
    "\n",
    "An example will explain this better.\n",
    "\n",
    "Let's say, we train the model using the first 1000 integers. To create a binary representation of fixed length, we first need to find the maximum length of the input vector. 2^10 is 1024. So, we need the input vector to be of length 10.\n",
    "\n",
    "The output is encoded as 0, 1, 2, 3 for \"fizzbuzz\", \"buzz\", \"fizz\" and \"the given number\" respectively.\n",
    "\n",
    "**Examples**\n",
    "\n",
    "*Example 1*\n",
    "\n",
    "Input: 100  \n",
    "Output: \"buzz\"  \n",
    "\n",
    "The input feature: 0001100100  \n",
    "Output label: 1  \n",
    "\n",
    "*Example 2*\n",
    "\n",
    "Input: 11  \n",
    "Output: \"the given number\"  \n",
    "\n",
    "The input feature: 0000001011   \n",
    "Output label: 3  \n",
    "\n",
    "Now, let's start building models using `mxnet`. Each digit of the input feature vector will be an input neuron. The output will have 4 neurons, one corresponding to each of the classes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate the data**\n",
    "\n",
    "Let's now generate the data. Let's first import the required libraries `numpy` and `mxnet`.\n",
    "\n",
    "One way to install `mxnet` is to run the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#install mxnet. If its already available, upgrade it.\n",
    "!pip install mxnet --upgrade --pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import os\n",
    "mx.random.seed(1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `mxnet`, every array has a context - either on a GPU or CPU. \n",
    "\n",
    "The data size won't be too high for this exercise and CPU should suffice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the context to be CPU\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**helper functions**\n",
    "\n",
    "We need a few helper functions to start with.\n",
    "\n",
    "We need a function to convert the given input integer to its binary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to encode the integer to its binary representation\n",
    "def binary_encode(i, num_digits):\n",
    "    return np.array([i >> d & 1 for d in range(num_digits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function to encode the target into one of the 4 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to encode the target into multi-class\n",
    "def fizz_buzz_encode(i):\n",
    "    if   i % 15 == 0: \n",
    "        return 0\n",
    "    elif i % 5  == 0: \n",
    "        return 1\n",
    "    elif i % 3  == 0: \n",
    "        return 2\n",
    "    else:             \n",
    "        return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is built and we use it to predict it for a given number, we need a function that maps the prediction to the right output. The following function helps us do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given prediction, map it to the correct output label\n",
    "def fizz_buzz(i, prediction):\n",
    "    if prediction == 0:\n",
    "        return \"fizzbuzz\"\n",
    "    elif prediction == 1:\n",
    "        return \"buzz\"\n",
    "    elif prediction == 2:\n",
    "        return \"fizz\"\n",
    "    else:\n",
    "        return str(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create train, validation, test datasets**\n",
    "\n",
    "We now need to create the train, validation and test datasets.\n",
    "\n",
    "Let's generate 100,000 data points. That's the first 100,000 integers.\n",
    "\n",
    "In ideal case, the train, validation and test datasets are randomly selected. But for sake of simplicity, let's use the first 100 integers as test dataset. We won't use this for training. Once we create the model, we will check the accuracy of the model by predicting on the test dataset.\n",
    "\n",
    "We will use the integers 101 to 50,000 as training dataset. We will build the model using this dataset.\n",
    "\n",
    "We will use integers 50,001 to 100,000 as validation dataset. We could use this for hyper-parameter tuning. In this exercise, we wouldn't be doing hyper-parameter tuning though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start by defining the number of integers to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Number of integers to generate\n",
    "MAX_NUMBER = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the input feature vector has to be determined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The input feature vector is determined by NUM_DIGITS\n",
    "NUM_DIGITS = np.log2(MAX_NUMBER).astype(np.int)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train, test and validation datasets are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate training dataset - both features and labels\n",
    "trainX = np.array([binary_encode(i, NUM_DIGITS) for i in range(101, np.int(MAX_NUMBER/2))])\n",
    "trainY = np.array([fizz_buzz_encode(i)          for i in range(101, np.int(MAX_NUMBER/2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate validation dataset - both features and labels\n",
    "valX = np.array([binary_encode(i, NUM_DIGITS) for i in range(np.int(MAX_NUMBER/2), MAX_NUMBER)])\n",
    "valY = np.array([fizz_buzz_encode(i)          for i in range(np.int(MAX_NUMBER/2), MAX_NUMBER)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate test dataset - both features and labels\n",
    "testX = np.array([binary_encode(i, NUM_DIGITS) for i in range(1, 101)])\n",
    "testY = np.array([fizz_buzz_encode(i)          for i in range(1, 101)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created the train, test and validation datasets, let's use `mxnet's` iterator `NDArrayIter`. It allows us to specify batch size for training and also allows us to set a flag to shuffle the data or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the parameters\n",
    "batch_size = 100\n",
    "num_inputs = NUM_DIGITS\n",
    "num_outputs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create iterator for train, test and validation datasets\n",
    "train_data = mx.io.NDArrayIter(trainX, trainY,\n",
    "                               batch_size, shuffle=True)\n",
    "val_data = mx.io.NDArrayIter(valX, valY,\n",
    "                               batch_size, shuffle=True)\n",
    "test_data = mx.io.NDArrayIter(testX, testY,\n",
    "                              batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to write another helper function. This one is to evaluate the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to evaluate accuracy of the model\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    data_iterator.reset()\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return predictions,acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression from scratch**\n",
    "\n",
    "Now that we have the data and the helper functions in place, let's go ahead and build logistic regression from scratch. [This](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter02_supervised-learning/softmax-regression-scratch.ipynb) article does a wonderful job of building a multi-class logistic regression from scratch using `mxnet`. We will follow a similar template for our problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mxnet` comes with a `autograd` package. `autograd` enables automatic differentiation of NDArray operations and is used to compute the gradients of the loss function with respect to the model weights. More about `autograd` can be found [here](https://mxnet.incubator.apache.org/api/python/autograd.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import autograd package\n",
    "from mxnet import autograd, nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to initialize the bias and weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize the weight and bias matrix\n",
    "\n",
    "#weights matrix\n",
    "W = nd.random_normal(shape=(num_inputs, num_outputs))\n",
    "#bias matrix\n",
    "b = nd.random_normal(shape=num_outputs)\n",
    "\n",
    "#Model parameters\n",
    "params = [W, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to allocate space for each parameter's gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the output to be the probability for each of the classes. The sum of the probabilities should sum upto one. This is accomplished by the softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(y_linear):\n",
    "    exp = nd.exp(y_linear-nd.max(y_linear))\n",
    "    norms = nd.sum(exp, axis=0, exclude=True).reshape((-1,1))\n",
    "    return exp / norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function we will be using is softmax cross entropy. Cross-entropy maximizes the log-likelihood given to the correct labels. More about softmax cross-entropy can be read [here](https://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss function\n",
    "def softmax_cross_entropy(yhat, y):\n",
    "    return - nd.nansum(y * nd.log(yhat), axis=0, exclude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    y_linear = nd.dot(X, W) + b\n",
    "    yhat = softmax(y_linear)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model to learn(update) the model parameters(weights and biases), we need to define an optimizer. Stochastic Gradient Descent is one of the most popular methods to update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the optimizer\n",
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets execute the training loops. For each training step, batch_size determines the number of data points that will be passed through the network to learn. Once all data points are passed through the network, one epoch is completed. The parameter `epochs` define the number of times the entire dataset has to be cycled through the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyper paramters for the training\n",
    "epochs = 100\n",
    "learning_rate = .01\n",
    "smoothing_constant = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now execute the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99. Loss: 1.18811465231, Train_acc 0.532825651303, Val_acc 0.5332\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    #at the start of each epoch, the train data iterator is reset\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 4)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0)) \n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "    #the training and validation accuracies are computed\n",
    "    _,val_accuracy = evaluate_accuracy(val_data, net)\n",
    "    _,train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Val_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 100 epochs, the train accuracy and validation accuracy doesn't seem to be great. It is only around 53%. Let's see what's the accuracy on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16'\n",
      " '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29' '30' '31'\n",
      " '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43' '44' '45' '46'\n",
      " '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57' '58' '59' '60' '61'\n",
      " '62' '63' '64' '65' '66' '67' '68' '69' '70' '71' '72' '73' '74' '75' '76'\n",
      " '77' '78' '79' '80' '81' '82' '83' '84' '85' '86' '87' '88' '89' '90' '91'\n",
      " '92' '93' '94' '95' '96' '97' '98' '99' '100']\n",
      "Test Accuracy :  0.53\n"
     ]
    }
   ],
   "source": [
    "#model accuracy on the test dataset\n",
    "predictions,test_accuracy = evaluate_accuracy(test_data, net)\n",
    "output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "print(output)\n",
    "print(\"Test Accuracy : \",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the test dataset is also 53%. The model doesn't seem to be working well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look into doing the same using `Gluon`\n",
    "\n",
    "**What is `Gluon`?**\n",
    "\n",
    "Gluon package is a high-level interface for MXNet. Gluon supports both imperative and symbolic programming, making it easy to train complex models imperatively. More details can be found [here](https://mxnet.incubator.apache.org/api/python/gluon.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-layer perceptron using `Gluon`**\n",
    "\n",
    "Let's build a multi-layer perceptron(MLP) model using `gluon`. \n",
    "\n",
    "MLP is one of the simplest deep learning models. More details can be found on [wikipedia](https://en.wikipedia.org/wiki/Multilayer_perceptron)\n",
    "![](img/MLP.jpg)\n",
    "\n",
    "[image source](http://file.scirp.org/Html/2-2200704_39386.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to import `gluon`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import gluon\n",
    "from mxnet import gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reset the training, test and validation iterators\n",
    "train_data.reset()\n",
    "val_data.reset()\n",
    "test_data.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define the Gluon Sequential Model. Each hidden layer is added sequentially. the `num_hidden` variable defines the number of neurons in each of the hidden layers. The `relu` activation function is used in each of the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define number of neurons in each hidden layer\n",
    "num_hidden = 64\n",
    "#Define the sequential network\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(num_inputs, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous exercise, where we built the model from scratch, the parameters need to be initialized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize parameters\n",
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is defined to be softmax cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define the loss function\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer is chosen to be stochastic gradient descent, similar to the previous model. For this model, we define both the learning rate and momentum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the optimizer\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .02,'momentum':0.9})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define variables/hyper-paramters\n",
    "epochs = 100\n",
    "moving_loss = 0.\n",
    "best_accuracy = 0.\n",
    "best_epoch = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 99. Loss: 0.021456909065, Train_acc 0.99498997996, Val_acc 0.44902\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "for e in range(epochs):\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            cross_entropy = loss(output, label)\n",
    "            cross_entropy.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        if i == 0:\n",
    "            moving_loss = nd.mean(cross_entropy).asscalar()\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * nd.mean(cross_entropy).asscalar()\n",
    "\n",
    "    _,val_accuracy = evaluate_accuracy(val_data, net)\n",
    "    _,train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    \n",
    "    if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            if best_epoch!=-1:\n",
    "                print('deleting previous checkpoint...')\n",
    "                os.remove('mlp-%d.params'%(best_epoch))\n",
    "            best_epoch = e\n",
    "            print('Best validation accuracy found. Checkpointing...')\n",
    "            net.save_params('mlp-%d.params'%(e))\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Val_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy looks good - but validation accuracy is terrible. This generally is a sign of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now predict it on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the parameters\n",
    "net.load_params('mlp-%d.params'%(best_epoch), ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' 'buzz' '6' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13' '14'\n",
      " '15' 'buzz' '17' 'fizz' '19' 'buzz' 'fizz' '22' '23' 'fizz' 'buzz' '26'\n",
      " 'fizz' '28' '29' 'fizzbuzz' '31' 'buzz' 'fizz' '34' 'buzz' 'fizz' '37'\n",
      " '38' '39' 'buzz' '41' 'fizz' '43' 'buzz' '45' 'buzz' '47' 'fizz' '49'\n",
      " 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59' 'fizz' 'buzz'\n",
      " '62' 'fizz' '64' 'buzz' '66' '67' '68' '69' 'buzz' '71' 'fizz' '73' '74'\n",
      " '75' '76' '77' '78' '79' 'buzz' 'fizz' '82' '83' 'fizz' 'buzz' '86' '87'\n",
      " '88' '89' 'fizzbuzz' '91' '92' 'fizz' '94' 'buzz' 'fizz' '97' '98' '99'\n",
      " 'buzz']\n",
      "Test Accuracy :  0.83\n"
     ]
    }
   ],
   "source": [
    "#predict on the test dataset\n",
    "predictions,test_accuracy = evaluate_accuracy(test_data, net)\n",
    "output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "print(output)\n",
    "print(\"Test Accuracy : \",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy is 83%. Let's try to explore another model architecture to see how the model accuracy is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional Neural Network**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "CNN is a deep learning architecture that's widely used in image recognition. \n",
    "\n",
    "The two key layers in a CNN architecture are the convolution layer and the pooling layer.\n",
    "\n",
    "The convolution layer applies a convolution operation to the input. Convolution is the process of adding each element of the image to its local neighbors, weighted by the kernel. More information can be found at [wikipedia](https://en.wikipedia.org/wiki/Kernel_(image_processing)\n",
    "\n",
    "The pooling layer combine the outputs of neuron clusters at one layer into a single neuron in the next layer. For example, if the method is to use max pooling and the neuron cluster is 2x2 - every 2x2 pixels is replaced by the maximum value amongst those 4 pixels.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to reshape the data for CNN:  \n",
    "(x_dim,y_dim) &rarr; (x_dim,#of channels = 1,y_dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reshape train, test and validation datasets\n",
    "trainX= trainX.reshape(trainX.shape[0],1,trainX.shape[1])\n",
    "valX= valX.reshape(valX.shape[0],1,valX.shape[1])\n",
    "testX= testX.reshape(testX.shape[0],1,testX.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the iterators for the reshaped train, test and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define the iterators\n",
    "train_data = mx.io.NDArrayIter(trainX, trainY,\n",
    "                               batch_size, shuffle=True)\n",
    "val_data = mx.io.NDArrayIter(valX, valY,\n",
    "                               batch_size, shuffle=True)\n",
    "test_data = mx.io.NDArrayIter(testX, testY,\n",
    "                              batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN model is now defined. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = mx.sym.var('data')\n",
    "#first convolutional layer\n",
    "#the kernel size is 2\n",
    "#relu activation function is used\n",
    "#max pooling is the next layer\n",
    "conv1 = mx.sym.Convolution(data=data, kernel=(2,), num_filter=20)\n",
    "tanh1 = mx.sym.Activation(data=conv1, act_type=\"relu\")\n",
    "pool1 = mx.sym.Pooling(data=tanh1, pool_type=\"max\", kernel=(2,), stride=(2,))\n",
    "# second convolutional layer\n",
    "conv2 = mx.sym.Convolution(data=pool1, kernel=(2,), num_filter=50)\n",
    "tanh2 = mx.sym.Activation(data=conv2, act_type=\"relu\")\n",
    "pool2 = mx.sym.Pooling(data=tanh2, pool_type=\"max\", kernel=(2,), stride=(2,))\n",
    "# first fully connected layer\n",
    "flatten = mx.sym.flatten(data=pool2)\n",
    "fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)\n",
    "tanh3 = mx.sym.Activation(data=fc1, act_type=\"relu\")\n",
    "# second fully connected layer\n",
    "fc2 = mx.sym.FullyConnected(data=tanh3, num_hidden=num_outputs)\n",
    "# softmax loss\n",
    "lenet = mx.sym.SoftmaxOutput(data=fc2, name='softmax')\n",
    "cnn_model = mx.mod.Module(symbol=lenet, context=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above looks like a lot of code and might be overwhelming if this is the first time you are looking at a CNN model. A good way to understand what's happening is by visualizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: plot Pages: 1 -->\n",
       "<svg width=\"214pt\" height=\"1100pt\"\n",
       " viewBox=\"0.00 0.00 214.00 1100.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 1096)\">\n",
       "<title>plot</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-1096 210,-1096 210,4 -4,4\"/>\n",
       "<!-- data -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>data</title>\n",
       "<ellipse fill=\"#8dd3c7\" stroke=\"#000000\" cx=\"47\" cy=\"-29\" rx=\"47\" ry=\"29\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-24.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">data</text>\n",
       "</g>\n",
       "<!-- convolution4 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>convolution4</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"#000000\" points=\"94,-152 0,-152 0,-94 94,-94 94,-152\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-125.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Convolution</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2/1, 20</text>\n",
       "</g>\n",
       "<!-- convolution4&#45;&gt;data -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>convolution4&#45;&gt;data</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47,-83.6321C47,-75.1148 47,-66.2539 47,-58.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47,-93.7731 42.5001,-83.773 47,-88.7731 47.0001,-83.7731 47.0001,-83.7731 47.0001,-83.7731 47,-88.7731 51.5001,-83.7731 47,-93.7731 47,-93.7731\"/>\n",
       "</g>\n",
       "<!-- activation6 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>activation6</title>\n",
       "<polygon fill=\"#ffffb3\" stroke=\"#000000\" points=\"94,-246 0,-246 0,-188 94,-188 94,-246\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-219.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Activation</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-205.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">relu</text>\n",
       "</g>\n",
       "<!-- activation6&#45;&gt;convolution4 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>activation6&#45;&gt;convolution4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47,-177.6321C47,-169.1148 47,-160.2539 47,-152.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47,-187.7731 42.5001,-177.773 47,-182.7731 47.0001,-177.7731 47.0001,-177.7731 47.0001,-177.7731 47,-182.7731 51.5001,-177.7731 47,-187.7731 47,-187.7731\"/>\n",
       "</g>\n",
       "<!-- pooling4 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>pooling4</title>\n",
       "<polygon fill=\"#80b1d3\" stroke=\"#000000\" points=\"94,-340 0,-340 0,-282 94,-282 94,-340\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-313.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Pooling</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-299.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">max, 2/2</text>\n",
       "</g>\n",
       "<!-- pooling4&#45;&gt;activation6 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>pooling4&#45;&gt;activation6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47,-271.6321C47,-263.1148 47,-254.2539 47,-246.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47,-281.7731 42.5001,-271.773 47,-276.7731 47.0001,-271.7731 47.0001,-271.7731 47.0001,-271.7731 47,-276.7731 51.5001,-271.7731 47,-281.7731 47,-281.7731\"/>\n",
       "</g>\n",
       "<!-- convolution5 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>convolution5</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"#000000\" points=\"94,-434 0,-434 0,-376 94,-376 94,-434\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-407.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Convolution</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-393.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2/1, 50</text>\n",
       "</g>\n",
       "<!-- convolution5&#45;&gt;pooling4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>convolution5&#45;&gt;pooling4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47,-365.6321C47,-357.1148 47,-348.2539 47,-340.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47,-375.7731 42.5001,-365.773 47,-370.7731 47.0001,-365.7731 47.0001,-365.7731 47.0001,-365.7731 47,-370.7731 51.5001,-365.7731 47,-375.7731 47,-375.7731\"/>\n",
       "</g>\n",
       "<!-- activation7 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>activation7</title>\n",
       "<polygon fill=\"#ffffb3\" stroke=\"#000000\" points=\"94,-528 0,-528 0,-470 94,-470 94,-528\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-501.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Activation</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-487.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">relu</text>\n",
       "</g>\n",
       "<!-- activation7&#45;&gt;convolution5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>activation7&#45;&gt;convolution5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47,-459.6321C47,-451.1148 47,-442.2539 47,-434.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47,-469.7731 42.5001,-459.773 47,-464.7731 47.0001,-459.7731 47.0001,-459.7731 47.0001,-459.7731 47,-464.7731 51.5001,-459.7731 47,-469.7731 47,-469.7731\"/>\n",
       "</g>\n",
       "<!-- pooling5 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>pooling5</title>\n",
       "<polygon fill=\"#80b1d3\" stroke=\"#000000\" points=\"94,-622 0,-622 0,-564 94,-564 94,-622\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-595.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Pooling</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-581.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">max, 2/2</text>\n",
       "</g>\n",
       "<!-- pooling5&#45;&gt;activation7 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>pooling5&#45;&gt;activation7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47,-553.6321C47,-545.1148 47,-536.2539 47,-528.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47,-563.7731 42.5001,-553.773 47,-558.7731 47.0001,-553.7731 47.0001,-553.7731 47.0001,-553.7731 47,-558.7731 51.5001,-553.7731 47,-563.7731 47,-563.7731\"/>\n",
       "</g>\n",
       "<!-- flatten2 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>flatten2</title>\n",
       "<polygon fill=\"#fdb462\" stroke=\"#000000\" points=\"94,-716 0,-716 0,-658 94,-658 94,-716\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-682.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">flatten2</text>\n",
       "</g>\n",
       "<!-- flatten2&#45;&gt;pooling5 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>flatten2&#45;&gt;pooling5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47,-647.6321C47,-639.1148 47,-630.2539 47,-622.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47,-657.7731 42.5001,-647.773 47,-652.7731 47.0001,-647.7731 47.0001,-647.7731 47.0001,-647.7731 47,-652.7731 51.5001,-647.7731 47,-657.7731 47,-657.7731\"/>\n",
       "</g>\n",
       "<!-- fullyconnected3 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>fullyconnected3</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"#000000\" points=\"94,-810 0,-810 0,-752 94,-752 94,-810\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-783.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">FullyConnected</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-769.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">500</text>\n",
       "</g>\n",
       "<!-- fullyconnected3&#45;&gt;flatten2 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>fullyconnected3&#45;&gt;flatten2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47,-741.6321C47,-733.1148 47,-724.2539 47,-716.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47,-751.7731 42.5001,-741.773 47,-746.7731 47.0001,-741.7731 47.0001,-741.7731 47.0001,-741.7731 47,-746.7731 51.5001,-741.7731 47,-751.7731 47,-751.7731\"/>\n",
       "</g>\n",
       "<!-- activation8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>activation8</title>\n",
       "<polygon fill=\"#ffffb3\" stroke=\"#000000\" points=\"94,-904 0,-904 0,-846 94,-846 94,-904\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-877.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Activation</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-863.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">relu</text>\n",
       "</g>\n",
       "<!-- activation8&#45;&gt;fullyconnected3 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>activation8&#45;&gt;fullyconnected3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47,-835.6321C47,-827.1148 47,-818.2539 47,-810.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47,-845.7731 42.5001,-835.773 47,-840.7731 47.0001,-835.7731 47.0001,-835.7731 47.0001,-835.7731 47,-840.7731 51.5001,-835.7731 47,-845.7731 47,-845.7731\"/>\n",
       "</g>\n",
       "<!-- fullyconnected4 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>fullyconnected4</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"#000000\" points=\"94,-998 0,-998 0,-940 94,-940 94,-998\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-971.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">FullyConnected</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-957.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4</text>\n",
       "</g>\n",
       "<!-- fullyconnected4&#45;&gt;activation8 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>fullyconnected4&#45;&gt;activation8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47,-929.6321C47,-921.1148 47,-912.2539 47,-904.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47,-939.7731 42.5001,-929.773 47,-934.7731 47.0001,-929.7731 47.0001,-929.7731 47.0001,-929.7731 47,-934.7731 51.5001,-929.7731 47,-939.7731 47,-939.7731\"/>\n",
       "</g>\n",
       "<!-- softmax_label -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>softmax_label</title>\n",
       "<ellipse fill=\"#8dd3c7\" stroke=\"#000000\" cx=\"159\" cy=\"-969\" rx=\"47\" ry=\"29\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-964.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">softmax_label</text>\n",
       "</g>\n",
       "<!-- softmax -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>softmax</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"#000000\" points=\"150,-1092 56,-1092 56,-1034 150,-1034 150,-1092\"/>\n",
       "<text text-anchor=\"middle\" x=\"103\" y=\"-1058.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">softmax</text>\n",
       "</g>\n",
       "<!-- softmax&#45;&gt;fullyconnected4 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>softmax&#45;&gt;fullyconnected4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M80.3812,-1025.0328C75.0542,-1016.091 69.4571,-1006.6959 64.401,-998.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"85.5882,-1033.7731 76.6042,-1027.4852 83.0292,-1029.4776 80.4701,-1025.1821 80.4701,-1025.1821 80.4701,-1025.1821 83.0292,-1029.4776 84.3361,-1022.8789 85.5882,-1033.7731 85.5882,-1033.7731\"/>\n",
       "</g>\n",
       "<!-- softmax&#45;&gt;softmax_label -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>softmax&#45;&gt;softmax_label</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M125.5712,-1025.1126C131.3182,-1015.4659 137.3825,-1005.2865 142.7449,-996.2854\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"120.4118,-1033.7731 121.6639,-1022.8789 122.9708,-1029.4776 125.5299,-1025.1821 125.5299,-1025.1821 125.5299,-1025.1821 122.9708,-1029.4776 129.3958,-1027.4852 120.4118,-1033.7731 120.4118,-1033.7731\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x10bd1f9b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.viz.plot_network(lenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the CNN model. This is similar to the `sklearn` interface. We call the `fit` function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training the CNN model\n",
    "cnn_model.fit(train_data,\n",
    "                eval_data=val_data,\n",
    "                optimizer='sgd',\n",
    "                optimizer_params={'learning_rate':0.01,'momentum':0.9},\n",
    "                eval_metric='acc',\n",
    "                num_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what the test accuracy is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16'\n",
      " '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29' '30' '31'\n",
      " '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43' '44' '45' '46'\n",
      " '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57' '58' '59' '60' '61'\n",
      " '62' '63' '64' '65' '66' '67' '68' '69' '70' '71' '72' '73' '74' '75' '76'\n",
      " '77' '78' '79' '80' '81' '82' '83' '84' '85' '86' '87' '88' '89' '90' '91'\n",
      " '92' '93' '94' '95' '96' '97' '98' '99' '100']\n",
      "Test Accuracy :  0.53\n"
     ]
    }
   ],
   "source": [
    "#Test accuracy of the CNN model\n",
    "acc = mx.metric.Accuracy()\n",
    "cnn_model.score(test_data, acc)\n",
    "probabilities = cnn_model.predict(test_data)\n",
    "predictions = nd.argmax(probabilities, axis=1)\n",
    "output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "print(output)\n",
    "print(\"Test Accuracy : \",acc.get_name_value()[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, this model has overfitting and all the output are only the actual numbers. The three classes - \"fizz\", \"buzz\" and \"fizzbuzz\" are missing from the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion and Next Steps**\n",
    "\n",
    "This article gave a quick overview of how to get started with `mxnet` and `gluon`. With `gluon`, prototyping and experimenting with various architectures is much faster. `autograd` allows us to record computation history on the fly so as to calculate gradients later. \n",
    "\n",
    "[The straight dope](https://github.com/zackchase/mxnet-the-straight-dope/) has a good set of tutorials to get started with `mxnet`.\n",
    "\n",
    "While this article briefly touched upon Convolutional Neural Networks, it didn't provide enough motivation and real-life use cases. While we saw models overfitting, we didn't get into how to overcome overfitting. In the next article, we will show how to build models to do image recognition with some real-life use cases.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
