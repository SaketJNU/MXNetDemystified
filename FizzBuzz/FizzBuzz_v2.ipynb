{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncovering Hidden Patterns Through Machine Learning: Lessons from **FizzBuzz** for `MXNet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When data scientist Joel Grus wrote an [article](http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/) on using machine learning to solve the *fizzbuzz* problem last year, most people saw it as an exercise in comedy, perhaps with a warning about the inappropriate use of AI. But we saw a deeper lesson. Certainly, you don’t need AI to solve *fizzbuzz* so long as someone tells you the algorithm underlying the problem. But suppose you discover a seemingly random pattern like *fizzbuzz* output in nature? Patterns like that exist throughout real life, and no one gives us the algorithm. Machine learning solves such problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This summer, I had an opportunity to interview with an AI -startup that I really liked. And guess what? I was asked to solve *fizzbuzz* using deep learning. Long story short, I didn't get the job offer !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this made us think about why *fizzbuzz* makes sense as an application of deep learning. On the surface, it’s a silly problem in integer arithmetic (or number theory, if you like to be pedantic). But it generates interesting patterns, and if you saw a list of inputs and outputs without knowing the underlying algorithm, finding a way to predict the outputs would be hard. Therefore, *fizzbuzz* is an easy way to generate patterns on which you can test deep learning techniques. In this article, we’ll try the popular `MXNet` tools (Amazon.com’s default offering for AWS machine learning) and find that this little exercise takes more effort than one might expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is fizzbuzz ?**\n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Fizz_buzz), *fizzbuzz* originated as children’s game, but for a long time has been a popular challenge that interviewers give programming candidates. Given an integer x, the programmer has to produce output according to the following rules:\n",
    "\n",
    "Given an integer `x`, the output is determined by the following rules:\n",
    "\n",
    "- if `x` is divisible by 3, output is \"fizz\"\n",
    "- if `x` is divisible by 5, output is \"buzz\"\n",
    "- if `x` is divisible by 15, output is \"fizzbuzz\"\n",
    "- else, the output is `x`\n",
    "\n",
    "A typical output sequence will look like this\n",
    "\n",
    "| Input   |      Output      | \n",
    "|----------|:-------------:|\n",
    "| 1 |  1 |\n",
    "| 2 |  2 |\n",
    "| 3 | \"fizz\" |\n",
    "| 4 | 4 |\n",
    "| 5 | \"buzz\" |\n",
    "| 6 | \"fizz\" |\n",
    "| 7 | 7 |\n",
    "| 8 | 8 |\n",
    "| 9 | \"fizz\" |\n",
    "| 10 | \"buzz\" |\n",
    "| 11 | 11 |\n",
    "| 12 | \"fizz\" |\n",
    "| 13 | 13 |\n",
    "| 14 | 14 |\n",
    "| 15 | \"fizzbuzz\" |\n",
    "| 16 | 16 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The requirements generate a surprisingly complex state machine, so they can reveal whether a novice programming candidate has good organizational skills. But if we know the rules that generate the data, there’s really no need for machine learning. Unfortunately, in real-life, we only have the data. Machine learning helps us create a model of the data. In this aspect, *fizzbuzz* provides us with an easy-to-understand dataset and allows us to understand and explore the algorithms.\n",
    "\n",
    "What follows below is a pedantic exercise in understanding how `MXNet` can be used to solve the *fizzbuzz* problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is `MXNet`?**\n",
    "\n",
    "`MXNet` is a scalable open-source deep learning framework with a [broad base of support](https://www.oreilly.com/ideas/apache-mxnetthe-fruit-of-cross-institutional-collaboration). It is supported by Amazon, Intel, Dato, Baidu, Microsoft, and MIT among others. \n",
    "\n",
    "`MXNet` offer the following [advantages](https://mxnet.incubator.apache.org/get_started/why_mxnet.html):\n",
    "\n",
    "**Device Placement**: It is easy to specify where each data structure should live (CPU vs GPU).  \n",
    "**Multi-GPU training**: It is easy to scale the computations with more GPUs.  \n",
    "**Automatic differentiation**: It automates the derivative calculations.  \n",
    "**Optimized Predefined Layers**: The pre-defined layers are optimized for speed.\n",
    "\n",
    "We use it in this article because of its popularity and because it is one of the most flexible frameworks in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure of the article**\n",
    "\n",
    "In the subsequent sections, we will do the following\n",
    "\n",
    "1. Structure the problem as a multi-class classification problem\n",
    "2. Generate the *fizzbuzz* data\n",
    "3. Divide the data into train and test\n",
    "4. Build a logistic regression model in `MXNet` from scratch\n",
    "5. Introduce `Gluon`\n",
    "6. Build a multi-layer-perceptron model using `Gluon`\n",
    "\n",
    "The examples are coded in Python because it is compact and well-known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure the problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data, how do we structure this as a machine learning problem? To do supervised machine learning, we need features and a target variable. *Fizzbuzz* can be modeled as a multi-class classification problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input**: Let’s think of a way to do feature engineering for the input. The input is an integer. One option, which Joel Grus employed in his article, is convert the number to its binary representation. The binary representation can be fixed-length and each digit of the fixed-length binary representation can be an input feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target**: The target can be one of the four classes - *fizz*, *buzz*, *fizzbuzz* or *the given number*. The model should predict which of the classes is most likely for an input number. After the four classes are encoded and the model is built, it will return one of four prediction labels. So we will also need a decoder function to convert the label to the corresponding output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example will explain this better.\n",
    "\n",
    "Let’s say, we train the model using the first 1,000 integers. To create a binary representation of fixed length, we first need to find the maximum length of the input vector. All numbers up to 1,000 can be represented in binary within the size 2<sup>10</sup> (which is 1024), so, we need the input vector to be of length 10.\n",
    "\n",
    "The output is encoded in the prediction labels as 0, 1, 2, 3 for “fizzbuzz”, “buzz”, “fizz” and “the given number” respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: 11  \n",
    "Output: 11\n",
    "\n",
    "<img src=\"img/11_new.jpg\" width=\"450\" height=\"450\" />\n",
    "\n",
    "*Figure 1. Input and Output encoding*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Each digit of the input feature vector will be an input neuron. The output will have 4 neurons, one corresponding to each of the labels.\n",
    "\n",
    "Figure 1 depicts this process.\n",
    "\n",
    "<img src=\"img/fizzbuzz_arch.jpg\" width=\"650\" height=\"350\" />\n",
    "\n",
    "*Figure 2. Data encoding/decoding architecture*\n",
    "\n",
    "Now let’s start building models using `MXNet`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing and configuring MXNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the software we need for this example, let’s first import the required Python libraries `numpy` and `mxnet`.\n",
    "\n",
    "One way to install `mxnet` is to run the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#install mxnet. If its already available, upgrade it.\n",
    "!pip install mxnet --upgrade --pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import os\n",
    "mx.random.seed(1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `mxnet`, every array has a context - either on a GPU or CPU. \n",
    "\n",
    "The data size won't be too high for this exercise and a single CPU should suffice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the context to be CPU\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper functions**\n",
    "\n",
    "We need a few helper functions to start with.\n",
    "\n",
    "First, a function to convert the given input integer to its binary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to encode the integer to its binary representation\n",
    "def binary_encode(i, num_digits):\n",
    "    return np.array([i >> d & 1 for d in range(num_digits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a function to encode the target into one of the four classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to encode the target into multi-class\n",
    "def fizz_buzz_encode(i):\n",
    "    if   i % 15 == 0: \n",
    "        return 0\n",
    "    elif i % 5  == 0: \n",
    "        return 1\n",
    "    elif i % 3  == 0: \n",
    "        return 2\n",
    "    else:             \n",
    "        return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is built and we use it to predict the prediction label for a given number, we need a decoder function: a function that maps the prediction to the right output. The following function helps us do that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given prediction, map it to the correct output label\n",
    "def fizz_buzz(i, prediction):\n",
    "    if prediction == 0:\n",
    "        return \"fizzbuzz\"\n",
    "    elif prediction == 1:\n",
    "        return \"buzz\"\n",
    "    elif prediction == 2:\n",
    "        return \"fizz\"\n",
    "    else:\n",
    "        return str(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating train, validation, test datasets**\n",
    "\n",
    "We now need to create the train, validation, and test datasets.\n",
    "\n",
    "Let’s generate 100,000 data points, representing the integers from 1 to 100,000.\n",
    "\n",
    "In ideal case, the train, validation and test datasets are randomly selected. But for sake of simplicity, let’s use the first 100 integers as test dataset. (Because we know how *fizzbuzz* works, we know that the output values are distributed pretty evenly across all integers, so we don’t have to worry about where we sample test data.) Once we create the model, we will check the accuracy of the model by predicting on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the integers 101 to 50,000--essentially the bottom half of the data range--as the training dataset. We will build the model using this dataset.\n",
    "\n",
    "We will use integers 50,001 to 100,000--the top half of the data range--as the validation dataset. We could use this for hyper-parameter tuning. In this exercise, though, we won’t be doing hyper-parameter tuning.\n",
    "\n",
    "We start by defining the number of integers to generate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Number of integers to generate\n",
    "MAX_NUMBER = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number also determines the length of the input feature vector. We determine the number of bits required for the binary representation, adding 1 because we round down the logarithm to the next lower integer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The input feature vector is determined by NUM_DIGITS\n",
    "NUM_DIGITS = np.log2(MAX_NUMBER).astype(np.int)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train, test and validation datasets are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate training dataset - both features and labels\n",
    "trainX = np.array([binary_encode(i, NUM_DIGITS) for i in range(101, np.int(MAX_NUMBER/2))])\n",
    "trainY = np.array([fizz_buzz_encode(i)          for i in range(101, np.int(MAX_NUMBER/2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate validation dataset - both features and labels\n",
    "valX = np.array([binary_encode(i, NUM_DIGITS) for i in range(np.int(MAX_NUMBER/2), MAX_NUMBER)])\n",
    "valY = np.array([fizz_buzz_encode(i)          for i in range(np.int(MAX_NUMBER/2), MAX_NUMBER)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate test dataset - both features and labels\n",
    "testX = np.array([binary_encode(i, NUM_DIGITS) for i in range(1, 101)])\n",
    "testY = np.array([fizz_buzz_encode(i)          for i in range(1, 101)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve created the train, test and validation datasets, let’s load them into instances of `mxnet`'s iterator `NDArrayIter`. It allows us to specify batch size for training and to set a flag to shuffle the data or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the parameters\n",
    "batch_size = 100\n",
    "num_inputs = NUM_DIGITS\n",
    "num_outputs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create iterator for train, test and validation datasets\n",
    "train_data = mx.io.NDArrayIter(trainX, trainY,\n",
    "                               batch_size, shuffle=True)\n",
    "val_data = mx.io.NDArrayIter(valX, valY,\n",
    "                               batch_size, shuffle=True)\n",
    "test_data = mx.io.NDArrayIter(testX, testY,\n",
    "                              batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to write another helper function to evaluate the accuracy of the model. After the predictions are generated, this function uses `MXNet` methods to return a floating-point number from 0 to 1 that indicates how well the function holds up against the validation data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to evaluate accuracy of the model\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    data_iterator.reset()\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return predictions,acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression From Scratch**\n",
    "\n",
    "We have the data and the helper functions in place.\n",
    "\n",
    "To understand a plain-vanilla neural network, let's build a multi-class logistic regression model from scratch using `MXNet`. To build from scratch, this is what we need to do:\n",
    "\n",
    "1. Initialize weights and bias to random values\n",
    "2. Compute the forward pass\n",
    "    - For each observation, the input features and weights are multiplied, added with bias and passed onto the sigmoid function. That's the predicted output for that input. \n",
    "3. Compute the error.\n",
    "4. Update the weights and bias using gradient descent\n",
    "5. Repeat steps 2-4 until convergence or for a specific number of epochs\n",
    "\n",
    "\n",
    "[This](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter02_supervised-learning/softmax-regression-scratch.ipynb) article does a wonderful job of building a multi-class logistic regression from scratch using `MXNet`. \n",
    "\n",
    "`mxnet` comes with an [`autograd`](https://mxnet.incubator.apache.org/api/python/autograd.html) package, which enables automatic differentiation of NDArray operations and is used to compute the gradients of the loss function with respect to the model weights. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import autograd package\n",
    "from mxnet import autograd, nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first programming step is to initialize the bias and weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize the weight and bias matrix\n",
    "\n",
    "#weights matrix\n",
    "W = nd.random_normal(shape=(num_inputs, num_outputs))\n",
    "#bias matrix\n",
    "b = nd.random_normal(shape=num_outputs)\n",
    "\n",
    "#Model parameters\n",
    "params = [W, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to allocate memory for each parameter's gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the output to be the probability for each of the classes. The sum of the probabilities should sum to one, which we can get by running the following softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(y_linear):\n",
    "    exp = nd.exp(y_linear-nd.max(y_linear))\n",
    "    norms = nd.sum(exp, axis=0, exclude=True).reshape((-1,1))\n",
    "    return exp / norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function we will be using is softmax cross entropy. Cross-entropy maximizes the log-likelihood given to the correct labels. More about softmax cross-entropy can be read [here](https://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss function\n",
    "def softmax_cross_entropy(yhat, y):\n",
    "    return - nd.nansum(y * nd.log(yhat), axis=0, exclude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    y_linear = nd.dot(X, W) + b\n",
    "    yhat = softmax(y_linear)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model to learn(update) the model parameters(weights and biases), we need to define an optimizer. Stochastic Gradient Descent is one of the most popular methods to update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the optimizer\n",
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets execute the training loops. For each training step, batch_size determines the number of data points that will be passed through the network for it to learn. Once all data points are passed through the network, one epoch is completed. The parameter epochs define the number of times the entire dataset has to be cycled through the training process. We’ll choose a fairly large number of epochs so we give the framework a good chance to learn the pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyper parameters for the training\n",
    "epochs = 100\n",
    "learning_rate = .01\n",
    "smoothing_constant = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now execute the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99. Loss: 1.18811465231, Train_acc 0.532825651303, Val_acc 0.5332\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    #at the start of each epoch, the train data iterator is reset\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 4)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0)) \n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "    #the training and validation accuracies are computed\n",
    "    _,val_accuracy = evaluate_accuracy(val_data, net)\n",
    "    _,train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Val_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 100 epochs, the train accuracy and validation accuracy doesn't seem great. It is only around 53%. Let's check the accuracy on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16'\n",
      " '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29' '30' '31'\n",
      " '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43' '44' '45' '46'\n",
      " '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57' '58' '59' '60' '61'\n",
      " '62' '63' '64' '65' '66' '67' '68' '69' '70' '71' '72' '73' '74' '75' '76'\n",
      " '77' '78' '79' '80' '81' '82' '83' '84' '85' '86' '87' '88' '89' '90' '91'\n",
      " '92' '93' '94' '95' '96' '97' '98' '99' '100']\n",
      "Test Accuracy :  0.53\n"
     ]
    }
   ],
   "source": [
    "#model accuracy on the test dataset\n",
    "predictions,test_accuracy = evaluate_accuracy(test_data, net)\n",
    "output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "print(output)\n",
    "print(\"Test Accuracy : \",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the test dataset is also 53%. The model doesn't seem to be working well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is `Gluon`?**\n",
    "\n",
    "So it’s time to roll out some more powerful weapons. Let’s look into doing the same machine learning using `Gluon`.\n",
    "\n",
    "The `Gluon` package is a high-level interface for `MXNet` that supports both imperative and symbolic programming, making it easy to train complex models imperatively. More details can be found [here](https://mxnet.incubator.apache.org/api/python/gluon.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-layer perceptron using `Gluon`**\n",
    "\n",
    "Let's build a multi-layer perceptron(MLP) model using `gluon`. \n",
    "\n",
    "MLP is one of the simplest deep learning models. It runs data through multiple layers, using feedback to refine the processing layers (see Figure 3). More details can be found on [wikipedia](https://en.wikipedia.org/wiki/Multilayer_perceptron).\n",
    "\n",
    "<img src=\"img/MLP_new.jpg\" width=\"650\" height=\"650\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 3. Architecture of a small multi-layer perceptron*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to import `gluon`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import gluon\n",
    "from mxnet import gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reset the training, test and validation iterators\n",
    "train_data.reset()\n",
    "val_data.reset()\n",
    "test_data.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define the Gluon Sequential Model. Each hidden layer is added sequentially. the `num_hidden` variable defines the number of neurons in each of the hidden layers. The `relu` activation function is used in each of the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define number of neurons in each hidden layer\n",
    "num_hidden = 64\n",
    "#Define the sequential network\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(num_inputs, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous exercise, where we built the model from scratch, the parameters need to be initialized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize parameters\n",
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is defined, as in the previous example, to be softmax cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define the loss function\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen optimizer is stochastic gradient descent, similar to the previous model. For this model, we define both the learning rate and momentum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the optimizer\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .02,'momentum':0.9})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define variables/hyper-paramters\n",
    "epochs = 100\n",
    "moving_loss = 0.\n",
    "best_accuracy = 0.\n",
    "best_epoch = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 99. Loss: 0.021456909065, Train_acc 0.99498997996, Val_acc 0.44902\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "for e in range(epochs):\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            cross_entropy = loss(output, label)\n",
    "            cross_entropy.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        if i == 0:\n",
    "            moving_loss = nd.mean(cross_entropy).asscalar()\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * nd.mean(cross_entropy).asscalar()\n",
    "\n",
    "    _,val_accuracy = evaluate_accuracy(val_data, net)\n",
    "    _,train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    \n",
    "    if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            if best_epoch!=-1:\n",
    "                print('deleting previous checkpoint...')\n",
    "                os.remove('mlp-%d.params'%(best_epoch))\n",
    "            best_epoch = e\n",
    "            print('Best validation accuracy found. Checkpointing...')\n",
    "            net.save_params('mlp-%d.params'%(e))\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Val_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy looks good--but validation accuracy is terrible. This generally is a sign of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now predict the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the parameters\n",
    "net.load_params('mlp-%d.params'%(best_epoch), ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' 'buzz' '6' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13' '14'\n",
      " '15' 'buzz' '17' 'fizz' '19' 'buzz' 'fizz' '22' '23' 'fizz' 'buzz' '26'\n",
      " 'fizz' '28' '29' 'fizzbuzz' '31' 'buzz' 'fizz' '34' 'buzz' 'fizz' '37'\n",
      " '38' '39' 'buzz' '41' 'fizz' '43' 'buzz' '45' 'buzz' '47' 'fizz' '49'\n",
      " 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59' 'fizz' 'buzz'\n",
      " '62' 'fizz' '64' 'buzz' '66' '67' '68' '69' 'buzz' '71' 'fizz' '73' '74'\n",
      " '75' '76' '77' '78' '79' 'buzz' 'fizz' '82' '83' 'fizz' 'buzz' '86' '87'\n",
      " '88' '89' 'fizzbuzz' '91' '92' 'fizz' '94' 'buzz' 'fizz' '97' '98' '99'\n",
      " 'buzz']\n",
      "Test Accuracy :  0.83\n"
     ]
    }
   ],
   "source": [
    "#predict on the test dataset\n",
    "predictions,test_accuracy = evaluate_accuracy(test_data, net)\n",
    "output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "print(output)\n",
    "print(\"Test Accuracy : \",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy is 83%. \n",
    "\n",
    "Adding more hidden layers led to an improvement of the accuracy. It is possible to experiment with more epochs and/or more hidden layers with different activation functions to see if accuracy improves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion and Next Steps**\n",
    "\n",
    "This article gave a quick overview of how to get started with `mxnet` and `gluon`. With `gluon`, prototyping and experimenting with various architectures is much faster. `autograd` allows us to record computation history on the fly so as to calculate gradients later. \n",
    "\n",
    "And we’ve seen that *fizzbuzz* is more complex than it seems at first, and provides a nice basis for studying different machine learning approaches.\n",
    "\n",
    "[The straight dope](https://github.com/zackchase/mxnet-the-straight-dope/) has a good set of tutorials to get started with `mxnet`.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
