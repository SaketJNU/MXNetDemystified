{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grokking **FizzBuzz** using `MXNet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last year, Joel Grus wrote a brilliant [article](http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/) on *fizzbuzz*. He had attended an interview and was asked about *fizzbuzz*. He went on to solve it using `tensorflow`. Now, you may wonder why should we use deep learning for this? Atleast, I thought that way and didn't really pay much attention to the code. \n",
    "\n",
    "This summer, I had an opportunity to interview with an AI-startup that I really liked. And guess what? I was asked to solve `fizzbuzz` using deep learning. Long story short, neither Joel nor I got the job ! \n",
    "\n",
    "But this made me think about why `fizzbuzz` makes sense? But before we get on to that, what is this `fizzbuzz` problem?\n",
    "\n",
    "**What is fizzbuzz**\n",
    "\n",
    "Given an integer `x`, the output is determined by the following rules:\n",
    "\n",
    "- if `x` is divisible by 3, output is \"fizz\"\n",
    "- if `x` is divisible by 5, output is \"buzz\"\n",
    "- if `x` is divisible by 15, output is \"fizzbuzz\"\n",
    "- else, the output is `x`\n",
    "\n",
    "A typical output sequence will look like this\n",
    "\n",
    "| Input   |      Output      | \n",
    "|----------|:-------------:|\n",
    "| 1 |  1 |\n",
    "| 2 |  2 |\n",
    "| 3 | \"fizz\" |\n",
    "| 4 | 4 |\n",
    "| 5 | \"buzz\" |\n",
    "| 6 | \"fizz\" |\n",
    "| 7 | 7 |\n",
    "| 8 | 8 |\n",
    "| 9 | \"fizz\" |\n",
    "| 10 | \"buzz\" |\n",
    "| 11 | 11 |\n",
    "| 12 | \"fizz\" |\n",
    "| 13 | 13 |\n",
    "| 14 | 14 |\n",
    "| 15 | \"fizzbuzz\" |\n",
    "| 16 | 16 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know the rules that generate the data, there's really no need for machine learning. Unfortunately, in real-life, we only have the data. The goal of machine learning is to learn the function that generated the data. In this aspect, `fizzbuzz` provides us with an easy-to-understand dataset and allows us to understand and explore the algorithms better. \n",
    "\n",
    "What follows below is a pedantic exercise in understanding how `MXNet` can be used to solve the `fizzbuzz` problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is `MXNet`**\n",
    "\n",
    "`MXNet` is a scalable open-source deep learning framework. It scales to multiple GPUs and multiple machines. At Amazon, `MXNet` is the deep learning framework of choice at AWS. It is supported by Intel, Dato, Baidu, Microsoft, MIT amongst others. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's the structure of the article?**\n",
    "\n",
    "In the subsequent sections, we will do the following\n",
    "\n",
    "1. Generate the fizzbuzz data\n",
    "2. Divide the data into train and test\n",
    "3. Structure the problem as a multi-class classification problem\n",
    "4. Build a logistic regression model in `MXNet` from scratch\n",
    "5. Build a logistic regression model using `MXNet`\n",
    "6. Introduce `Gluon`\n",
    "7. Build a multi-layer-perceptron model using `Gluon`\n",
    "8. Build a Convolutional Neural Network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import nd\n",
    "import os\n",
    "mx.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a function to encode the integer to its binary representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_encode(i, num_digits):\n",
    "    return np.array([i >> d & 1 for d in range(num_digits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a function to label the data and map the labels back to categorical strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fizz_buzz_encode(i):\n",
    "    if   i % 15 == 0: \n",
    "        return 0\n",
    "    elif i % 5  == 0: \n",
    "        return 1\n",
    "    elif i % 3  == 0: \n",
    "        return 2\n",
    "    else:             \n",
    "        return 3\n",
    "    \n",
    "def fizz_buzz(i, prediction):\n",
    "    if prediction == 0:\n",
    "        return \"fizzbuzz\"\n",
    "    elif prediction == 1:\n",
    "        return \"buzz\"\n",
    "    elif prediction == 2:\n",
    "        return \"fizz\"\n",
    "    else:\n",
    "        return str(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the Numpy NdArray for training, validation and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NUMBER = 100000\n",
    "NUM_DIGITS = np.log2(MAX_NUMBER).astype(np.int)+1\n",
    "trainX = np.array([binary_encode(i, NUM_DIGITS) for i in range(101, np.int(MAX_NUMBER/2))])\n",
    "trainY = np.array([fizz_buzz_encode(i)          for i in range(101, np.int(MAX_NUMBER/2))])\n",
    "valX = np.array([binary_encode(i, NUM_DIGITS) for i in range(np.int(MAX_NUMBER/2), MAX_NUMBER)])\n",
    "valY = np.array([fizz_buzz_encode(i)          for i in range(np.int(MAX_NUMBER/2), MAX_NUMBER)])\n",
    "testX = np.array([binary_encode(i, NUM_DIGITS) for i in range(1, 101)])\n",
    "testY = np.array([fizz_buzz_encode(i)          for i in range(1, 101)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create mxnet NDarrayiter for training, validation and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_inputs = NUM_DIGITS\n",
    "num_outputs = 4\n",
    "train_data = mx.io.NDArrayIter(trainX, trainY,\n",
    "                               batch_size, shuffle=True)\n",
    "val_data = mx.io.NDArrayIter(valX, valY,\n",
    "                               batch_size, shuffle=True)\n",
    "test_data = mx.io.NDArrayIter(testX, testY,\n",
    "                              batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets define the function to calculate accuracy of a model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    data_iterator.reset()\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return predictions,acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the bias and weight matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_scale = .01\n",
    "\n",
    "W = nd.random_normal(shape=(num_inputs, num_outputs))\n",
    "b = nd.random_normal(shape=num_outputs)\n",
    "\n",
    "params = [W, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Allocate space for each parameter's gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall pass our $yhat\\_linear$ and compute the softmax and its log all at once inside the $softmax\\_cross\\_entropy$ loss function simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_cross_entropy(yhat_linear, y):\n",
    "    return - nd.nansum(y * nd.log_softmax(yhat_linear), axis=0, exclude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    y_linear = nd.dot(X, W) + b\n",
    "    return y_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets execute the training loops**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99. Loss: 1.18811465231, Train_acc 0.532825651303, Val_acc 0.5332\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "learning_rate = .01\n",
    "smoothing_constant = .01\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 4)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0)) \n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "    _,val_accuracy = evaluate_accuracy(val_data, net)\n",
    "    _,train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Val_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets see what the model predicts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16'\n",
      " '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29' '30' '31'\n",
      " '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43' '44' '45' '46'\n",
      " '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57' '58' '59' '60' '61'\n",
      " '62' '63' '64' '65' '66' '67' '68' '69' '70' '71' '72' '73' '74' '75' '76'\n",
      " '77' '78' '79' '80' '81' '82' '83' '84' '85' '86' '87' '88' '89' '90' '91'\n",
      " '92' '93' '94' '95' '96' '97' '98' '99' '100']\n",
      "Test Accuracy :  0.53\n"
     ]
    }
   ],
   "source": [
    "predictions,test_accuracy = evaluate_accuracy(test_data, net)\n",
    "output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "print(output)\n",
    "print(\"Test Accuracy : \",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiLayer Perceptron using Gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets reset the Training, Validation and the Test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.reset()\n",
    "val_data.reset()\n",
    "test_data.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Gluon Sequential Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden = 64\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(num_inputs, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize Parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax Cross Entropy Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .02,'momentum':0.9})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets Train the MLP model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 99. Loss: 0.021456909065, Train_acc 0.99498997996, Val_acc 0.44902\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "moving_loss = 0.\n",
    "best_accuracy = 0.\n",
    "best_epoch = -1\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            cross_entropy = loss(output, label)\n",
    "            cross_entropy.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        if i == 0:\n",
    "            moving_loss = nd.mean(cross_entropy).asscalar()\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * nd.mean(cross_entropy).asscalar()\n",
    "\n",
    "    _,val_accuracy = evaluate_accuracy(val_data, net)\n",
    "    _,train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    \n",
    "    if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            if best_epoch!=-1:\n",
    "                print('deleting previous checkpoint...')\n",
    "                os.remove('mlp-%d.params'%(best_epoch))\n",
    "            best_epoch = e\n",
    "            print('Best validation accuracy found. Checkpointing...')\n",
    "            net.save_params('mlp-%d.params'%(e))\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Val_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets see what the model predicts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.load_params('mlp-%d.params'%(best_epoch), ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' 'buzz' '6' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13' '14'\n",
      " '15' 'buzz' '17' 'fizz' '19' 'buzz' 'fizz' '22' '23' 'fizz' 'buzz' '26'\n",
      " 'fizz' '28' '29' 'fizzbuzz' '31' 'buzz' 'fizz' '34' 'buzz' 'fizz' '37'\n",
      " '38' '39' 'buzz' '41' 'fizz' '43' 'buzz' '45' 'buzz' '47' 'fizz' '49'\n",
      " 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59' 'fizz' 'buzz'\n",
      " '62' 'fizz' '64' 'buzz' '66' '67' '68' '69' 'buzz' '71' 'fizz' '73' '74'\n",
      " '75' '76' '77' '78' '79' 'buzz' 'fizz' '82' '83' 'fizz' 'buzz' '86' '87'\n",
      " '88' '89' 'fizzbuzz' '91' '92' 'fizz' '94' 'buzz' 'fizz' '97' '98' '99'\n",
      " 'buzz']\n",
      "Test Accuracy :  0.83\n"
     ]
    }
   ],
   "source": [
    "predictions,test_accuracy = evaluate_accuracy(test_data, net)\n",
    "output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "print(output)\n",
    "print(\"Test Accuracy : \",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN using mxnet symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets reshape the data (x_dim,y_dim) &rarr; (x_dim,#of channels = 1,y_dim)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX= trainX.reshape(trainX.shape[0],1,trainX.shape[1])\n",
    "valX= valX.reshape(valX.shape[0],1,valX.shape[1])\n",
    "testX= testX.reshape(testX.shape[0],1,testX.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare the NDArrayIters corresponding to Training, Testing and Validation data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = mx.io.NDArrayIter(trainX, trainY,\n",
    "                               batch_size, shuffle=True)\n",
    "val_data = mx.io.NDArrayIter(valX, valY,\n",
    "                               batch_size, shuffle=True)\n",
    "test_data = mx.io.NDArrayIter(testX, testY,\n",
    "                              batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = mx.sym.var('data')\n",
    "# first conv layer\n",
    "conv1 = mx.sym.Convolution(data=data, kernel=(2,), num_filter=20)\n",
    "tanh1 = mx.sym.Activation(data=conv1, act_type=\"relu\")\n",
    "pool1 = mx.sym.Pooling(data=tanh1, pool_type=\"max\", kernel=(2,), stride=(2,))\n",
    "# second conv layer\n",
    "conv2 = mx.sym.Convolution(data=pool1, kernel=(2,), num_filter=50)\n",
    "tanh2 = mx.sym.Activation(data=conv2, act_type=\"relu\")\n",
    "pool2 = mx.sym.Pooling(data=tanh2, pool_type=\"max\", kernel=(2,), stride=(2,))\n",
    "# first fullc layer\n",
    "flatten = mx.sym.flatten(data=pool2)\n",
    "fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)\n",
    "tanh3 = mx.sym.Activation(data=fc1, act_type=\"relu\")\n",
    "# second fullc\n",
    "fc2 = mx.sym.FullyConnected(data=tanh3, num_hidden=num_outputs)\n",
    "# softmax loss\n",
    "lenet = mx.sym.SoftmaxOutput(data=fc2, name='softmax')\n",
    "cnn_model = mx.mod.Module(symbol=lenet, context=ctx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_model.fit(train_data,\n",
    "                eval_data=val_data,\n",
    "                optimizer='sgd',\n",
    "                optimizer_params={'learning_rate':0.01,'momentum':0.9},\n",
    "                eval_metric='acc',\n",
    "                num_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets see what the model predicts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16'\n",
      " '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29' '30' '31'\n",
      " '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43' '44' '45' '46'\n",
      " '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57' '58' '59' '60' '61'\n",
      " '62' '63' '64' '65' '66' '67' '68' '69' '70' '71' '72' '73' '74' '75' '76'\n",
      " '77' '78' '79' '80' '81' '82' '83' '84' '85' '86' '87' '88' '89' '90' '91'\n",
      " '92' '93' '94' '95' '96' '97' '98' '99' '100']\n",
      "Test Accuracy :  0.53\n"
     ]
    }
   ],
   "source": [
    "acc = mx.metric.Accuracy()\n",
    "cnn_model.score(test_data, acc)\n",
    "probabilities = cnn_model.predict(test_data)\n",
    "predictions = nd.argmax(probabilities, axis=1)\n",
    "output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "print(output)\n",
    "print(\"Test Accuracy : \",acc.get_name_value()[0][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
