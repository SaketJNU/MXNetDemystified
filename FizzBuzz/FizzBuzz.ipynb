{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import nd\n",
    "import os\n",
    "mx.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to encode the integer to its binary representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_encode(i, num_digits):\n",
    "    return np.array([i >> d & 1 for d in range(num_digits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to label the data and map the labels back to categorical strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizz_buzz_encode(i):\n",
    "    if   i % 15 == 0: \n",
    "        return 0\n",
    "    elif i % 5  == 0: \n",
    "        return 1\n",
    "    elif i % 3  == 0: \n",
    "        return 2\n",
    "    else:             \n",
    "        return 3\n",
    "    \n",
    "def fizz_buzz(i, prediction):\n",
    "    if prediction == 0:\n",
    "        return \"fizzbuzz\"\n",
    "    elif prediction == 1:\n",
    "        return \"buzz\"\n",
    "    elif prediction == 2:\n",
    "        return \"fizz\"\n",
    "    else:\n",
    "        return str(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Numpy NdArray for training, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUMBER = 20000\n",
    "NUM_DIGITS = np.log2(MAX_NUMBER).astype(np.int)+1\n",
    "trainX = np.array([binary_encode(i, NUM_DIGITS) for i in range(101, np.int(MAX_NUMBER/2))])\n",
    "trainY = np.array([fizz_buzz_encode(i)          for i in range(101, np.int(MAX_NUMBER/2))])\n",
    "valX = np.array([binary_encode(i, NUM_DIGITS) for i in range(np.int(MAX_NUMBER/2), MAX_NUMBER)])\n",
    "valY = np.array([fizz_buzz_encode(i)          for i in range(np.int(MAX_NUMBER/2), MAX_NUMBER)])\n",
    "testX = np.array([binary_encode(i, NUM_DIGITS) for i in range(1, 101)])\n",
    "testY = np.array([fizz_buzz_encode(i)          for i in range(1, 101)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create mxnet NDarrayiter for training, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_inputs = NUM_DIGITS\n",
    "num_outputs = 4\n",
    "train_data = mx.io.NDArrayIter(trainX, trainY,\n",
    "                               batch_size, shuffle=True)\n",
    "val_data = mx.io.NDArrayIter(valX, valY,\n",
    "                               batch_size, shuffle=True)\n",
    "test_data = mx.io.NDArrayIter(testX, testY,\n",
    "                              batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets define the function to calculate accuracy of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    data_iterator.reset()\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return predictions,acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the bias and weight matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_scale = .01\n",
    "\n",
    "W = nd.random_normal(shape=(num_inputs, num_outputs))\n",
    "b = nd.random_normal(shape=num_outputs)\n",
    "\n",
    "params = [W, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allocate space for each parameter's gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We shall pass our $yhat\\_linear$ and compute the softmax and its log all at once inside the $softmax\\_cross\\_entropy$ loss function simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy(yhat_linear, y):\n",
    "    return - nd.nansum(y * nd.log_softmax(yhat_linear), axis=0, exclude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    y_linear = nd.dot(X, W) + b\n",
    "    return y_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets execute the training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 2.38738988565, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 1. Loss: 1.61913783083, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 2. Loss: 1.33319039147, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 3. Loss: 1.22711897882, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 4. Loss: 1.18781804062, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 5. Loss: 1.17326428396, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 6. Loss: 1.16787579723, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 7. Loss: 1.16588060826, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 8. Loss: 1.16514169871, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 9. Loss: 1.16486794705, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 10. Loss: 1.164766432, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 11. Loss: 1.16472876077, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 12. Loss: 1.16471471864, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 13. Loss: 1.16470948928, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 14. Loss: 1.16470752959, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 15. Loss: 1.16470678703, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 16. Loss: 1.16470652181, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 17. Loss: 1.16470640311, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 18. Loss: 1.16470635518, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 19. Loss: 1.16470634922, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 20. Loss: 1.16470633963, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 21. Loss: 1.16470634012, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 22. Loss: 1.16470633854, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 23. Loss: 1.16470632348, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 24. Loss: 1.16470632506, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 25. Loss: 1.16470632263, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 26. Loss: 1.16470632307, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 27. Loss: 1.16470632664, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 28. Loss: 1.16470633754, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 29. Loss: 1.16470632947, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 30. Loss: 1.16470632933, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 31. Loss: 1.16470632377, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 32. Loss: 1.16470632193, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 33. Loss: 1.16470632238, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 34. Loss: 1.16470632514, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 35. Loss: 1.16470632596, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 36. Loss: 1.16470632574, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 37. Loss: 1.16470632042, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 38. Loss: 1.16470632291, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 39. Loss: 1.16470633287, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 40. Loss: 1.1647063214, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 41. Loss: 1.16470632016, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 42. Loss: 1.16470632292, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 43. Loss: 1.16470632402, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 44. Loss: 1.16470631598, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 45. Loss: 1.16470631593, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 46. Loss: 1.16470631531, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 47. Loss: 1.1647063241, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 48. Loss: 1.1647063276, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 49. Loss: 1.16470632571, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 50. Loss: 1.16470632598, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 51. Loss: 1.16470632352, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 52. Loss: 1.16470632244, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 53. Loss: 1.164706321, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 54. Loss: 1.16470632359, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 55. Loss: 1.16470631799, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 56. Loss: 1.1647063254, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 57. Loss: 1.16470632734, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 58. Loss: 1.16470632615, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 59. Loss: 1.16470632139, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 60. Loss: 1.16470632623, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 61. Loss: 1.16470631967, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 62. Loss: 1.16470632153, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 63. Loss: 1.16470633189, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 64. Loss: 1.16470632501, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 65. Loss: 1.16470632842, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 66. Loss: 1.16470632204, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 67. Loss: 1.16470632442, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 68. Loss: 1.16470631655, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 69. Loss: 1.16470632039, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 70. Loss: 1.16470631473, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 71. Loss: 1.16470631902, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 72. Loss: 1.16470632098, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 73. Loss: 1.16470632135, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 74. Loss: 1.16470632127, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 75. Loss: 1.164706317, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 76. Loss: 1.16470632445, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 77. Loss: 1.16470631473, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 78. Loss: 1.16470631785, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 79. Loss: 1.16470631391, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 80. Loss: 1.16470631874, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 81. Loss: 1.16470631992, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 82. Loss: 1.16470632005, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 83. Loss: 1.16470632422, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 84. Loss: 1.1647063221, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 85. Loss: 1.16470631297, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 86. Loss: 1.16470631872, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 87. Loss: 1.16470632564, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 88. Loss: 1.1647063222, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 89. Loss: 1.16470632227, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 90. Loss: 1.16470631917, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 91. Loss: 1.16470632141, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 92. Loss: 1.16470631998, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 93. Loss: 1.16470631441, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 94. Loss: 1.16470632389, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 95. Loss: 1.16470632115, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 96. Loss: 1.16470632159, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 97. Loss: 1.16470631962, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 98. Loss: 1.16470632017, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 99. Loss: 1.16470631647, Train_acc 0.533434343434, Val_acc 0.5334\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "learning_rate = .01\n",
    "smoothing_constant = .01\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx)#.reshape((-1, num_inputs))\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 4)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "        \n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0)) \n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "    _,val_accuracy = evaluate_accuracy(val_data, net)\n",
    "    _,train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Val_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets see what the model predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16'\n",
      " '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29' '30' '31'\n",
      " '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43' '44' '45' '46'\n",
      " '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57' '58' '59' '60' '61'\n",
      " '62' '63' '64' '65' '66' '67' '68' '69' '70' '71' '72' '73' '74' '75' '76'\n",
      " '77' '78' '79' '80' '81' '82' '83' '84' '85' '86' '87' '88' '89' '90' '91'\n",
      " '92' '93' '94' '95' '96' '97' '98' '99' '100']\n",
      "Test Accuracy :  0.53\n"
     ]
    }
   ],
   "source": [
    "predictions,test_accuracy = evaluate_accuracy(test_data, net)\n",
    "output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "print(output)\n",
    "print(\"Test Accuracy : \",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">MultiLayer Perceptron using Gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets reset the Training, Validation and the Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.reset()\n",
    "val_data.reset()\n",
    "test_data.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Gluon Sequestial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 64\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(num_inputs, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .02,'momentum':0.9})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Train the MLP model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 0. Loss: 1.25544475855, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 1. Loss: 1.12745423824, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 2. Loss: 1.12747068545, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 3. Loss: 1.12740531555, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 4. Loss: 1.12738983537, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 5. Loss: 1.12737592903, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 6. Loss: 1.12732159183, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 7. Loss: 1.12726641266, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 8. Loss: 1.12726239024, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 9. Loss: 1.12721520206, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 10. Loss: 1.1271914243, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 11. Loss: 1.1271304002, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 12. Loss: 1.12709254076, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 13. Loss: 1.12703109748, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 14. Loss: 1.12703689019, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 15. Loss: 1.12697660944, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 16. Loss: 1.12689947189, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 17. Loss: 1.12679029423, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 18. Loss: 1.12665554623, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 19. Loss: 1.12646663132, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 20. Loss: 1.12613216819, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 21. Loss: 1.12554655766, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 22. Loss: 1.12472167696, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 23. Loss: 1.12368480855, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 24. Loss: 1.12219723115, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 25. Loss: 1.11967905626, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 26. Loss: 1.11587696638, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 27. Loss: 1.11134891944, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 28. Loss: 1.10477431103, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 29. Loss: 1.0946488891, Train_acc 0.533333333333, Val_acc 0.5334\n",
      "Epoch 30. Loss: 1.07995190772, Train_acc 0.536161616162, Val_acc 0.5329\n",
      "Epoch 31. Loss: 1.0622996383, Train_acc 0.568888888889, Val_acc 0.5144\n",
      "Epoch 32. Loss: 1.04282260552, Train_acc 0.560101010101, Val_acc 0.5059\n",
      "Epoch 33. Loss: 1.01859332447, Train_acc 0.570101010101, Val_acc 0.5092\n",
      "Epoch 34. Loss: 0.965396784948, Train_acc 0.594646464646, Val_acc 0.5284\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 35. Loss: 0.892649279272, Train_acc 0.61202020202, Val_acc 0.5467\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 36. Loss: 0.849830283858, Train_acc 0.61898989899, Val_acc 0.553\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 37. Loss: 0.788529367575, Train_acc 0.627373737374, Val_acc 0.5556\n",
      "Epoch 38. Loss: 0.808862295055, Train_acc 0.603434343434, Val_acc 0.5185\n",
      "Epoch 39. Loss: 0.837125113293, Train_acc 0.601717171717, Val_acc 0.4885\n",
      "Epoch 40. Loss: 0.761308102992, Train_acc 0.63, Val_acc 0.5004\n",
      "Epoch 41. Loss: 0.736778043677, Train_acc 0.644949494949, Val_acc 0.5133\n",
      "Epoch 42. Loss: 0.722488669336, Train_acc 0.649696969697, Val_acc 0.5293\n",
      "Epoch 43. Loss: 0.710238883015, Train_acc 0.65101010101, Val_acc 0.5296\n",
      "Epoch 44. Loss: 0.698658058845, Train_acc 0.651414141414, Val_acc 0.5381\n",
      "Epoch 45. Loss: 0.691342838849, Train_acc 0.658080808081, Val_acc 0.5418\n",
      "Epoch 46. Loss: 0.686119708715, Train_acc 0.65797979798, Val_acc 0.5438\n",
      "Epoch 47. Loss: 0.682127028958, Train_acc 0.65797979798, Val_acc 0.5436\n",
      "Epoch 48. Loss: 0.678757853061, Train_acc 0.657575757576, Val_acc 0.5402\n",
      "Epoch 49. Loss: 0.673937810504, Train_acc 0.657676767677, Val_acc 0.539\n",
      "Epoch 50. Loss: 0.671040866632, Train_acc 0.654343434343, Val_acc 0.5331\n",
      "Epoch 51. Loss: 0.669036217399, Train_acc 0.653636363636, Val_acc 0.529\n",
      "Epoch 52. Loss: 0.666917086904, Train_acc 0.653939393939, Val_acc 0.5283\n",
      "Epoch 53. Loss: 0.664363911307, Train_acc 0.654747474747, Val_acc 0.5281\n",
      "Epoch 54. Loss: 0.661565881542, Train_acc 0.659898989899, Val_acc 0.5282\n",
      "Epoch 55. Loss: 0.658680906283, Train_acc 0.665858585859, Val_acc 0.5334\n",
      "Epoch 56. Loss: 0.655810914504, Train_acc 0.667474747475, Val_acc 0.5402\n",
      "Epoch 57. Loss: 0.652106593726, Train_acc 0.667878787879, Val_acc 0.549\n",
      "Epoch 58. Loss: 0.64944381606, Train_acc 0.668282828283, Val_acc 0.5504\n",
      "Epoch 59. Loss: 0.646774119166, Train_acc 0.668585858586, Val_acc 0.5505\n",
      "Epoch 60. Loss: 0.642781310014, Train_acc 0.669696969697, Val_acc 0.5489\n",
      "Epoch 61. Loss: 0.639082854568, Train_acc 0.668787878788, Val_acc 0.5508\n",
      "Epoch 62. Loss: 0.635487977922, Train_acc 0.668888888889, Val_acc 0.5514\n",
      "Epoch 63. Loss: 0.631047478686, Train_acc 0.66898989899, Val_acc 0.5521\n",
      "Epoch 64. Loss: 0.62633052049, Train_acc 0.66898989899, Val_acc 0.5423\n",
      "Epoch 65. Loss: 0.621855610505, Train_acc 0.669090909091, Val_acc 0.5338\n",
      "Epoch 66. Loss: 0.617499014766, Train_acc 0.672424242424, Val_acc 0.5335\n",
      "Epoch 67. Loss: 0.613292469768, Train_acc 0.675555555556, Val_acc 0.5385\n",
      "Epoch 68. Loss: 0.609574433254, Train_acc 0.683333333333, Val_acc 0.5415\n",
      "Epoch 69. Loss: 0.599491876455, Train_acc 0.688383838384, Val_acc 0.5443\n",
      "Epoch 70. Loss: 0.59303786796, Train_acc 0.697878787879, Val_acc 0.5383\n",
      "Epoch 71. Loss: 0.588495747098, Train_acc 0.712323232323, Val_acc 0.5477\n",
      "Epoch 72. Loss: 0.569047822274, Train_acc 0.733838383838, Val_acc 0.5528\n",
      "Epoch 73. Loss: 0.565437792385, Train_acc 0.738080808081, Val_acc 0.5502\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 74. Loss: 0.549914063529, Train_acc 0.758282828283, Val_acc 0.5703\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 75. Loss: 0.528259028559, Train_acc 0.77595959596, Val_acc 0.5732\n",
      "Epoch 76. Loss: 0.513849039199, Train_acc 0.804242424242, Val_acc 0.5731\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 77. Loss: 0.474236937807, Train_acc 0.822121212121, Val_acc 0.5842\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 78. Loss: 0.434695576031, Train_acc 0.829494949495, Val_acc 0.6014\n",
      "Epoch 79. Loss: 0.567782927414, Train_acc 0.80898989899, Val_acc 0.5731\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 80. Loss: 0.400997040473, Train_acc 0.900505050505, Val_acc 0.6405\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 81. Loss: 0.307354612506, Train_acc 0.906363636364, Val_acc 0.6433\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 82. Loss: 0.255855142952, Train_acc 0.914646464646, Val_acc 0.6478\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 83. Loss: 0.218824201981, Train_acc 0.949292929293, Val_acc 0.6652\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 84. Loss: 0.187798168323, Train_acc 0.956363636364, Val_acc 0.6725\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 85. Loss: 0.163815967241, Train_acc 0.963131313131, Val_acc 0.6813\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 86. Loss: 0.139166734529, Train_acc 0.970505050505, Val_acc 0.6897\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 87. Loss: 0.120410658573, Train_acc 0.970202020202, Val_acc 0.6983\n",
      "Epoch 88. Loss: 0.114223993237, Train_acc 0.972525252525, Val_acc 0.6946\n",
      "Epoch 89. Loss: 0.109207544467, Train_acc 0.97202020202, Val_acc 0.685\n",
      "Epoch 90. Loss: 0.102551238325, Train_acc 0.970808080808, Val_acc 0.6957\n",
      "Epoch 91. Loss: 0.0951915009207, Train_acc 0.978787878788, Val_acc 0.6852\n",
      "Epoch 92. Loss: 0.0910150352612, Train_acc 0.977272727273, Val_acc 0.6894\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 93. Loss: 0.0895613533142, Train_acc 0.976767676768, Val_acc 0.6989\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 94. Loss: 0.0861328877574, Train_acc 0.984242424242, Val_acc 0.6995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 95. Loss: 0.0715946130918, Train_acc 0.983737373737, Val_acc 0.7124\n",
      "Epoch 96. Loss: 0.0748032509177, Train_acc 0.98101010101, Val_acc 0.7044\n",
      "Epoch 97. Loss: 0.07565221132, Train_acc 0.982121212121, Val_acc 0.7006\n",
      "Epoch 98. Loss: 0.0570988757557, Train_acc 0.981818181818, Val_acc 0.6883\n",
      "Epoch 99. Loss: 0.055433691222, Train_acc 0.985858585859, Val_acc 0.6935\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "moving_loss = 0.\n",
    "best_accuracy = 0.\n",
    "best_epoch = -1\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            cross_entropy = loss(output, label)\n",
    "            cross_entropy.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        if i == 0:\n",
    "            moving_loss = nd.mean(cross_entropy).asscalar()\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * nd.mean(cross_entropy).asscalar()\n",
    "\n",
    "    _,val_accuracy = evaluate_accuracy(val_data, net)\n",
    "    _,train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    \n",
    "    if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            if best_epoch!=-1:\n",
    "                print('deleting previous checkpoint...')\n",
    "                os.remove('mlp-%d.params'%(best_epoch))\n",
    "            best_epoch = e\n",
    "            print('Best validation accuracy found. Checkpointing...')\n",
    "            net.save_params('mlp-%d.params'%(e))\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Val_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets see what the model predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_params('mlp-%d.params'%(best_epoch), ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' 'fizz' '4' 'buzz' 'fizz' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' '19' 'buzz' 'fizz' '22' '23' 'fizz'\n",
      " 'buzz' '26' 'fizz' '28' '29' 'fizzbuzz' '31' '32' 'fizz' '34' 'buzz'\n",
      " 'fizz' '37' '38' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47'\n",
      " 'fizz' '49' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' '61' '62' 'fizz' '64' 'buzz' 'fizz' '67' '68' 'fizz' 'buzz'\n",
      " '71' 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' 'fizz' '82'\n",
      " '83' 'fizz' 'buzz' '86' 'fizz' '88' '89' 'fizzbuzz' '91' '92' 'fizz' '94'\n",
      " 'buzz' 'fizz' '97' '98' 'fizz' 'buzz']\n",
      "Test Accuracy :  1.0\n"
     ]
    }
   ],
   "source": [
    "predictions,test_accuracy = evaluate_accuracy(test_data, net)\n",
    "output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "print(output)\n",
    "print(\"Test Accuracy : \",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">CNN using mxnet symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets reshape the data (x_dim,y_dim) &rarr; (x_dim,#of channels = 1,y_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX= trainX.reshape(trainX.shape[0],1,trainX.shape[1])\n",
    "valX= valX.reshape(valX.shape[0],1,valX.shape[1])\n",
    "testX= testX.reshape(testX.shape[0],1,testX.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the NDArrayIters corresponding to Training, Testing and Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mx.io.NDArrayIter(trainX, trainY,\n",
    "                               batch_size, shuffle=True)\n",
    "val_data = mx.io.NDArrayIter(valX, valY,\n",
    "                               batch_size, shuffle=True)\n",
    "test_data = mx.io.NDArrayIter(testX, testY,\n",
    "                              batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mx.sym.var('data')\n",
    "# first conv layer\n",
    "conv1 = mx.sym.Convolution(data=data, kernel=(2,), num_filter=20)\n",
    "tanh1 = mx.sym.Activation(data=conv1, act_type=\"relu\")\n",
    "pool1 = mx.sym.Pooling(data=tanh1, pool_type=\"max\", kernel=(2,), stride=(2,))\n",
    "# second conv layer\n",
    "conv2 = mx.sym.Convolution(data=pool1, kernel=(2,), num_filter=50)\n",
    "tanh2 = mx.sym.Activation(data=conv2, act_type=\"relu\")\n",
    "pool2 = mx.sym.Pooling(data=tanh2, pool_type=\"max\", kernel=(2,), stride=(2,))\n",
    "# first fullc layer\n",
    "flatten = mx.sym.flatten(data=pool2)\n",
    "fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)\n",
    "tanh3 = mx.sym.Activation(data=fc1, act_type=\"relu\")\n",
    "# second fullc\n",
    "fc2 = mx.sym.FullyConnected(data=tanh3, num_hidden=num_outputs)\n",
    "# softmax loss\n",
    "lenet = mx.sym.SoftmaxOutput(data=fc2, name='softmax')\n",
    "cnn_model = mx.mod.Module(symbol=lenet, context=ctx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.fit(train_data,\n",
    "                eval_data=val_data,\n",
    "                optimizer='sgd',\n",
    "                optimizer_params={'learning_rate':0.01,'momentum':0.9},\n",
    "                eval_metric='acc',\n",
    "                num_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets see what the model predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16'\n",
      " '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29' '30' '31'\n",
      " '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43' '44' '45' '46'\n",
      " '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57' '58' '59' '60' '61'\n",
      " '62' '63' '64' '65' '66' '67' '68' '69' '70' '71' '72' '73' '74' '75' '76'\n",
      " '77' '78' '79' '80' '81' '82' '83' '84' '85' '86' '87' '88' '89' '90' '91'\n",
      " '92' '93' '94' '95' '96' '97' '98' '99' '100']\n",
      "Test Accuracy :  0.53\n"
     ]
    }
   ],
   "source": [
    "acc = mx.metric.Accuracy()\n",
    "cnn_model.score(test_data, acc)\n",
    "probabilities = cnn_model.predict(test_data)\n",
    "predictions = nd.argmax(probabilities, axis=1)\n",
    "output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "print(output)\n",
    "print(\"Test Accuracy : \",acc.get_name_value()[0][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
