{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import nd\n",
    "\n",
    "mx.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to encode the integer to its binary representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_encode(i, num_digits):\n",
    "    return np.array([i >> d & 1 for d in range(num_digits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to label the data and map the labels back to categorical strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizz_buzz_encode(i):\n",
    "    if   i % 15 == 0: \n",
    "        return 0\n",
    "    elif i % 5  == 0: \n",
    "        return 1\n",
    "    elif i % 3  == 0: \n",
    "        return 2\n",
    "    else:             \n",
    "        return 3\n",
    "    \n",
    "def fizz_buzz(i, prediction):\n",
    "    if prediction == 0:\n",
    "        return \"fizzbuzz\"\n",
    "    elif prediction == 1:\n",
    "        return \"buzz\"\n",
    "    elif prediction == 2:\n",
    "        return \"fizz\"\n",
    "    else:\n",
    "        return str(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Numpy NdArray for training, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUMBER = 20000\n",
    "NUM_DIGITS = np.log2(MAX_NUMBER).astype(np.int)+1\n",
    "trainX = np.array([binary_encode(i, NUM_DIGITS) for i in range(101, np.int(MAX_NUMBER/2))])\n",
    "trainY = np.array([fizz_buzz_encode(i)          for i in range(101, np.int(MAX_NUMBER/2))])\n",
    "valX = np.array([binary_encode(i, NUM_DIGITS) for i in range(np.int(MAX_NUMBER/2), MAX_NUMBER)])\n",
    "valY = np.array([fizz_buzz_encode(i)          for i in range(np.int(MAX_NUMBER/2), MAX_NUMBER)])\n",
    "testX = np.array([binary_encode(i, NUM_DIGITS) for i in range(1, 101)])\n",
    "testY = np.array([fizz_buzz_encode(i)          for i in range(1, 101)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create mxnet NDarrayiter for training, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_inputs = NUM_DIGITS\n",
    "num_outputs = 4\n",
    "train_data = mx.io.NDArrayIter(trainX, trainY,\n",
    "                               batch_size, shuffle=True)\n",
    "val_data = mx.io.NDArrayIter(valX, valY,\n",
    "                               batch_size, shuffle=True)\n",
    "test_data = mx.io.NDArrayIter(testX, testY,\n",
    "                              batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets define the function to calculate accuracy of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    data_iterator.reset()\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return predictions,acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the bias and weight matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_scale = .01\n",
    "\n",
    "W = nd.random_normal(shape=(num_inputs, num_outputs))\n",
    "b = nd.random_normal(shape=num_outputs)\n",
    "\n",
    "params = [W, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allocate space for each parameter's gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We shall pass our $yhat\\_linear$ and compute the softmax and its log all at once inside the $softmax\\_cross\\_entropy$ loss function simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy(yhat_linear, y):\n",
    "    return - nd.nansum(y * nd.log_softmax(yhat_linear), axis=0, exclude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    y_linear = nd.dot(X, W) + b\n",
    "    return y_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets execute the training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 2.3163840174, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 1. Loss: 1.59599629381, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 2. Loss: 1.32759693647, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 3. Loss: 1.22796101584, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 4. Loss: 1.1910187941, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 5. Loss: 1.17732768185, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 6. Loss: 1.17225337721, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 7. Loss: 1.17037177288, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 8. Loss: 1.16967331976, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 9. Loss: 1.1694135974, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 10. Loss: 1.16931672309, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 11. Loss: 1.16928037754, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 12. Loss: 1.16926663325, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 13. Loss: 1.16926133941, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 14. Loss: 1.16925923994, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 15. Loss: 1.16925836518, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 16. Loss: 1.16925797809, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 17. Loss: 1.16925778551, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 18. Loss: 1.16925770603, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 19. Loss: 1.16925765285, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 20. Loss: 1.16925760513, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 21. Loss: 1.16925759981, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 22. Loss: 1.16925758399, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 23. Loss: 1.1692575864, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 24. Loss: 1.16925756453, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 25. Loss: 1.16925755058, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 26. Loss: 1.16925755093, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 27. Loss: 1.16925753628, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 28. Loss: 1.1692575324, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 29. Loss: 1.16925752881, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 30. Loss: 1.16925752992, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 31. Loss: 1.16925753493, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 32. Loss: 1.16925754735, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 33. Loss: 1.16925754706, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 34. Loss: 1.169257536, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 35. Loss: 1.16925753547, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 36. Loss: 1.16925753261, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 37. Loss: 1.16925753183, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 38. Loss: 1.16925753763, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 39. Loss: 1.16925753593, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 40. Loss: 1.16925753815, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 41. Loss: 1.16925752592, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 42. Loss: 1.16925753457, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 43. Loss: 1.16925753125, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 44. Loss: 1.16925753293, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 45. Loss: 1.16925754007, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 46. Loss: 1.16925754072, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 47. Loss: 1.16925754009, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 48. Loss: 1.16925753132, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 49. Loss: 1.16925753284, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 50. Loss: 1.16925753457, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 51. Loss: 1.16925753761, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 52. Loss: 1.16925753508, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 53. Loss: 1.16925754032, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 54. Loss: 1.16925753803, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 55. Loss: 1.16925753891, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 56. Loss: 1.16925753962, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 57. Loss: 1.16925753578, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 58. Loss: 1.16925753469, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 59. Loss: 1.16925753904, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 60. Loss: 1.16925753779, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 61. Loss: 1.16925753019, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 62. Loss: 1.16925752845, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 63. Loss: 1.16925753575, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 64. Loss: 1.16925753177, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 65. Loss: 1.16925753177, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 66. Loss: 1.16925753147, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 67. Loss: 1.16925753127, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 68. Loss: 1.1692575303, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 69. Loss: 1.1692575334, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 70. Loss: 1.16925752832, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 71. Loss: 1.16925753361, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 72. Loss: 1.16925753644, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 73. Loss: 1.16925752847, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 74. Loss: 1.16925753641, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 75. Loss: 1.16925753975, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 76. Loss: 1.16925754682, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 77. Loss: 1.16925754001, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 78. Loss: 1.1692575398, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 79. Loss: 1.16925752909, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 80. Loss: 1.16925754012, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 81. Loss: 1.16925753257, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 82. Loss: 1.16925753004, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 83. Loss: 1.16925753677, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 84. Loss: 1.16925754013, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 85. Loss: 1.16925753826, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 86. Loss: 1.16925753072, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 87. Loss: 1.1692575388, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 88. Loss: 1.16925753031, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 89. Loss: 1.16925752892, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 90. Loss: 1.16925753253, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 91. Loss: 1.16925753063, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 92. Loss: 1.16925754658, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 93. Loss: 1.16925753474, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 94. Loss: 1.16925754061, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 95. Loss: 1.16925752809, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 96. Loss: 1.16925753684, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 97. Loss: 1.1692575277, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 98. Loss: 1.16925753508, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 99. Loss: 1.16925754075, Train_acc 0.533434343434, Val_acc 0.5334\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "learning_rate = .01\n",
    "smoothing_constant = .01\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx)#.reshape((-1, num_inputs))\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 4)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "        \n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0)) \n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "    _,test_accuracy = evaluate_accuracy(val_data, net)\n",
    "    _,train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Val_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets see what the model predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16'\n",
      " '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29' '30' '31'\n",
      " '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43' '44' '45' '46'\n",
      " '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57' '58' '59' '60' '61'\n",
      " '62' '63' '64' '65' '66' '67' '68' '69' '70' '71' '72' '73' '74' '75' '76'\n",
      " '77' '78' '79' '80' '81' '82' '83' '84' '85' '86' '87' '88' '89' '90' '91'\n",
      " '92' '93' '94' '95' '96' '97' '98' '99' '100']\n",
      "Test Accuracy :  0.53\n"
     ]
    }
   ],
   "source": [
    "predictions,test_accuracy = evaluate_accuracy(test_data, net)\n",
    "output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "print(output)\n",
    "print(\"Test Accuracy : \",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">MultiLayer Perceptron using Gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets reset the Training, Validation and the Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.reset()\n",
    "val_data.reset()\n",
    "test_data.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Gluon Sequestial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 256\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(num_inputs, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .01,'momentum':0.9})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Train the MLP model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 1.25437421571, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 1. Loss: 1.15721442045, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 2. Loss: 1.15406116567, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 3. Loss: 1.15232871223, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 4. Loss: 1.15116667802, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 5. Loss: 1.15035152163, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 6. Loss: 1.14974839881, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 7. Loss: 1.14925333898, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 8. Loss: 1.1488747834, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 9. Loss: 1.14854265592, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 10. Loss: 1.1482677152, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 11. Loss: 1.14803200135, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 12. Loss: 1.14777964218, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 13. Loss: 1.14758265618, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 14. Loss: 1.14741772585, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 15. Loss: 1.14725668921, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 16. Loss: 1.14709102391, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 17. Loss: 1.14696401413, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 18. Loss: 1.14683521, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 19. Loss: 1.14673885372, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 20. Loss: 1.14661058165, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 21. Loss: 1.14649879169, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 22. Loss: 1.14640281038, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 23. Loss: 1.14630042302, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 24. Loss: 1.14622109157, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 25. Loss: 1.14612000851, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 26. Loss: 1.1460316191, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 27. Loss: 1.14592507507, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 28. Loss: 1.14583333824, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 29. Loss: 1.14573990166, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 30. Loss: 1.14562958301, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 31. Loss: 1.14550768355, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 32. Loss: 1.14536781804, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 33. Loss: 1.1452591547, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 34. Loss: 1.14518598216, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 35. Loss: 1.14511340318, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 36. Loss: 1.1450280253, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 37. Loss: 1.14493505665, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 38. Loss: 1.14486446781, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 39. Loss: 1.14478710404, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 40. Loss: 1.14470387948, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 41. Loss: 1.14461272948, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 42. Loss: 1.1445232298, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 43. Loss: 1.14443371441, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 44. Loss: 1.14434648209, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 45. Loss: 1.14422637874, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 46. Loss: 1.14411741251, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 47. Loss: 1.14397480615, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 48. Loss: 1.14386647337, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 49. Loss: 1.14372772075, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 50. Loss: 1.14352705376, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 51. Loss: 1.14331241312, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 52. Loss: 1.14304620115, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 53. Loss: 1.14277835242, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 54. Loss: 1.14243231838, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 55. Loss: 1.14204389519, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 56. Loss: 1.14155808484, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 57. Loss: 1.14102641544, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 58. Loss: 1.14032589689, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 59. Loss: 1.13944592995, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 60. Loss: 1.13838037717, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 61. Loss: 1.13714182609, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 62. Loss: 1.13568166456, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 63. Loss: 1.13383058555, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 64. Loss: 1.13139744892, Train_acc 0.533434343434, Val_acc 0.5329\n",
      "Epoch 65. Loss: 1.12878193325, Train_acc 0.533434343434, Val_acc 0.5309\n",
      "Epoch 66. Loss: 1.12521486864, Train_acc 0.533434343434, Val_acc 0.5263\n",
      "Epoch 67. Loss: 1.12187651698, Train_acc 0.533434343434, Val_acc 0.515\n",
      "Epoch 68. Loss: 1.11730930471, Train_acc 0.534444444444, Val_acc 0.5028\n",
      "Epoch 69. Loss: 1.11272972918, Train_acc 0.538787878788, Val_acc 0.4967\n",
      "Epoch 70. Loss: 1.10801571718, Train_acc 0.546868686869, Val_acc 0.5002\n",
      "Epoch 71. Loss: 1.1017943271, Train_acc 0.549090909091, Val_acc 0.4993\n",
      "Epoch 72. Loss: 1.09322995274, Train_acc 0.550909090909, Val_acc 0.4997\n",
      "Epoch 73. Loss: 1.08441858176, Train_acc 0.55202020202, Val_acc 0.4958\n",
      "Epoch 74. Loss: 1.07102411725, Train_acc 0.602727272727, Val_acc 0.5301\n",
      "Epoch 75. Loss: 1.05087162926, Train_acc 0.623333333333, Val_acc 0.5549\n",
      "Epoch 76. Loss: 1.0248055925, Train_acc 0.623535353535, Val_acc 0.5455\n",
      "Epoch 77. Loss: 0.997689116208, Train_acc 0.623434343434, Val_acc 0.5319\n",
      "Epoch 78. Loss: 0.967453647914, Train_acc 0.621515151515, Val_acc 0.5299\n",
      "Epoch 79. Loss: 0.94105259282, Train_acc 0.639797979798, Val_acc 0.533\n",
      "Epoch 80. Loss: 0.915006046926, Train_acc 0.658282828283, Val_acc 0.5334\n",
      "Epoch 81. Loss: 0.883997220829, Train_acc 0.669494949495, Val_acc 0.5309\n",
      "Epoch 82. Loss: 0.861439492143, Train_acc 0.692424242424, Val_acc 0.5341\n",
      "Epoch 83. Loss: 0.847178172874, Train_acc 0.700606060606, Val_acc 0.5354\n",
      "Epoch 84. Loss: 0.83382612491, Train_acc 0.713939393939, Val_acc 0.5302\n",
      "Epoch 85. Loss: 0.823786581468, Train_acc 0.722121212121, Val_acc 0.5326\n",
      "Epoch 86. Loss: 0.813247411734, Train_acc 0.734949494949, Val_acc 0.5433\n",
      "Epoch 87. Loss: 0.802838386662, Train_acc 0.738686868687, Val_acc 0.5425\n",
      "Epoch 88. Loss: 0.790252980846, Train_acc 0.741111111111, Val_acc 0.5426\n",
      "Epoch 89. Loss: 0.77721486858, Train_acc 0.74303030303, Val_acc 0.5427\n",
      "Epoch 90. Loss: 0.764408498986, Train_acc 0.744949494949, Val_acc 0.5439\n",
      "Epoch 91. Loss: 0.748996053447, Train_acc 0.747373737374, Val_acc 0.5462\n",
      "Epoch 92. Loss: 0.733293871127, Train_acc 0.747373737374, Val_acc 0.5509\n",
      "Epoch 93. Loss: 0.717770993032, Train_acc 0.748787878788, Val_acc 0.5573\n",
      "Epoch 94. Loss: 0.699593823643, Train_acc 0.750404040404, Val_acc 0.5582\n",
      "Epoch 95. Loss: 0.681704868806, Train_acc 0.755252525253, Val_acc 0.5614\n",
      "Epoch 96. Loss: 0.667096472249, Train_acc 0.760505050505, Val_acc 0.5638\n",
      "Epoch 97. Loss: 0.651891092433, Train_acc 0.765656565657, Val_acc 0.5666\n",
      "Epoch 98. Loss: 0.742968235246, Train_acc 0.734343434343, Val_acc 0.5266\n",
      "Epoch 99. Loss: 0.644503612785, Train_acc 0.768080808081, Val_acc 0.5782\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "moving_loss = 0.\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            cross_entropy = loss(output, label)\n",
    "            cross_entropy.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        if i == 0:\n",
    "            moving_loss = nd.mean(cross_entropy).asscalar()\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * nd.mean(cross_entropy).asscalar()\n",
    "\n",
    "    _,val_accuracy = evaluate_accuracy(val_data, net)\n",
    "    _,train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Val_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets see what the model predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' 'fizz' '4' '5' 'fizz' '7' '8' 'fizz' '10' '11' 'fizz' '13' '14'\n",
      " 'fizz' '16' '17' 'fizz' '19' '20' 'fizz' '22' '23' 'fizz' '25' '26' 'fizz'\n",
      " '28' '29' 'fizz' '31' '32' 'fizz' '34' '35' 'fizz' '37' '38' 'fizz' '40'\n",
      " '41' 'fizz' '43' '44' 'fizz' '46' '47' 'fizz' '49' '50' 'fizz' '52' '53'\n",
      " 'fizz' '55' '56' 'fizz' '58' '59' '60' '61' '62' 'fizz' '64' '65' 'fizz'\n",
      " '67' '68' 'fizz' '70' '71' 'fizz' '73' '74' 'fizz' '76' '77' 'fizz' '79'\n",
      " '80' 'fizz' '82' '83' 'fizz' '85' '86' 'fizz' '88' '89' 'fizz' '91' '92'\n",
      " 'fizz' '94' '95' 'fizz' '97' '98' 'fizz' '100']\n",
      "Test Accuracy :  0.8\n"
     ]
    }
   ],
   "source": [
    "predictions,test_accuracy = evaluate_accuracy(test_data, net)\n",
    "output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "print(output)\n",
    "print(\"Test Accuracy : \",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">CNN using mxnet symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets reshape the data (x_dim,y_dim) &rarr; (x_dim,#of channels = 1,y_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX= trainX.reshape(trainX.shape[0],1,trainX.shape[1])\n",
    "valX= valX.reshape(valX.shape[0],1,valX.shape[1])\n",
    "testX= testX.reshape(testX.shape[0],1,testX.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the NDArrayIters corresponding to Training, Testing and Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mx.io.NDArrayIter(trainX, trainY,\n",
    "                               batch_size, shuffle=True)\n",
    "val_data = mx.io.NDArrayIter(valX, valY,\n",
    "                               batch_size, shuffle=True)\n",
    "test_data = mx.io.NDArrayIter(testX, testY,\n",
    "                              batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mx.sym.var('data')\n",
    "# first conv layer\n",
    "conv1 = mx.sym.Convolution(data=data, kernel=(2,), num_filter=20)\n",
    "tanh1 = mx.sym.Activation(data=conv1, act_type=\"relu\")\n",
    "pool1 = mx.sym.Pooling(data=tanh1, pool_type=\"max\", kernel=(2,), stride=(2,))\n",
    "# second conv layer\n",
    "conv2 = mx.sym.Convolution(data=pool1, kernel=(2,), num_filter=50)\n",
    "tanh2 = mx.sym.Activation(data=conv2, act_type=\"relu\")\n",
    "pool2 = mx.sym.Pooling(data=tanh2, pool_type=\"max\", kernel=(2,), stride=(2,))\n",
    "# first fullc layer\n",
    "flatten = mx.sym.flatten(data=pool2)\n",
    "fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)\n",
    "tanh3 = mx.sym.Activation(data=fc1, act_type=\"relu\")\n",
    "# second fullc\n",
    "fc2 = mx.sym.FullyConnected(data=tanh3, num_hidden=num_outputs)\n",
    "# softmax loss\n",
    "lenet = mx.sym.SoftmaxOutput(data=fc2, name='softmax')\n",
    "cnn_model = mx.mod.Module(symbol=lenet, context=mx.cpu())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.fit(train_data,\n",
    "                eval_data=val_data,\n",
    "                optimizer='sgd',\n",
    "                optimizer_params={'learning_rate':0.01,'momentum':0.9},\n",
    "                eval_metric='acc',\n",
    "                num_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets see what the model predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16'\n",
      " '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29' '30' '31'\n",
      " '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43' '44' '45' '46'\n",
      " '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57' '58' '59' '60' '61'\n",
      " '62' '63' '64' '65' '66' '67' '68' '69' '70' '71' '72' '73' '74' '75' '76'\n",
      " '77' '78' '79' '80' '81' '82' '83' '84' '85' '86' '87' '88' '89' '90' '91'\n",
      " '92' '93' '94' '95' '96' '97' '98' '99' '100']\n",
      "Test Accuracy :  0.53\n"
     ]
    }
   ],
   "source": [
    "acc = mx.metric.Accuracy()\n",
    "cnn_model.score(test_data, acc)\n",
    "probabilities = cnn_model.predict(test_data)\n",
    "predictions = nd.argmax(probabilities, axis=1)\n",
    "output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "print(output)\n",
    "print(\"Test Accuracy : \",acc.get_name_value()[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
