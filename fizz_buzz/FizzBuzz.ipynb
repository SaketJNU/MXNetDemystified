{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import nd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to encode the integer to its binary representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_encode(i, num_digits):\n",
    "    return np.array([i >> d & 1 for d in range(num_digits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to label the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizz_buzz_encode(i):\n",
    "    if   i % 15 == 0: \n",
    "        return 0\n",
    "    elif i % 5  == 0: \n",
    "        return 1\n",
    "    elif i % 3  == 0: \n",
    "        return 2\n",
    "    else:             \n",
    "        return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Numpy NdArray for traing and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUMBER = 20000\n",
    "NUM_DIGITS = np.log2(MAX_NUMBER).astype(np.int)+1\n",
    "trainX = np.array([binary_encode(i, NUM_DIGITS) for i in range(101, np.int(MAX_NUMBER/2))])\n",
    "trainY = np.array([fizz_buzz_encode(i)          for i in range(101, np.int(MAX_NUMBER/2))])\n",
    "valX = np.array([binary_encode(i, NUM_DIGITS) for i in range(np.int(MAX_NUMBER/2), MAX_NUMBER)])\n",
    "valY = np.array([fizz_buzz_encode(i)          for i in range(np.int(MAX_NUMBER/2), MAX_NUMBER)])\n",
    "testX = np.array([binary_encode(i, NUM_DIGITS) for i in range(1, 101)])\n",
    "testY = np.array([fizz_buzz_encode(i)          for i in range(1, 101)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create mxnet NDarrayiter for training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_inputs = NUM_DIGITS\n",
    "num_outputs = 4\n",
    "train_data = mx.io.NDArrayIter(trainX, trainY,\n",
    "                               batch_size, shuffle=True)\n",
    "val_data = mx.io.NDArrayIter(valX, valY,\n",
    "                               batch_size, shuffle=True)\n",
    "test_data = mx.io.NDArrayIter(testX, testY,\n",
    "                              batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Gluon Sequestial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 256\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(num_inputs, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .1,'momentum':0.9})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the function for evaluating Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    data_iterator.reset()\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        data = batch.data[0].as_in_context(ctx).reshape((-1, num_inputs))\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Train the MLP model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 1.23673449719, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 1. Loss: 1.13576843941, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 2. Loss: 1.13470083972, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 3. Loss: 1.13372674407, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 4. Loss: 1.13286777237, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 5. Loss: 1.13115855296, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 6. Loss: 1.12697293319, Train_acc 0.533434343434, Val_acc 0.5334\n",
      "Epoch 7. Loss: 1.10770986514, Train_acc 0.526464646465, Val_acc 0.5327\n",
      "Epoch 8. Loss: 1.02883823208, Train_acc 0.635757575758, Val_acc 0.5602\n",
      "Epoch 9. Loss: 0.853434077768, Train_acc 0.678282828283, Val_acc 0.5504\n",
      "Epoch 10. Loss: 0.665760026595, Train_acc 0.74404040404, Val_acc 0.5515\n",
      "Epoch 11. Loss: 0.592008043265, Train_acc 0.780404040404, Val_acc 0.5956\n",
      "Epoch 12. Loss: 0.549476172342, Train_acc 0.807575757576, Val_acc 0.6096\n",
      "Epoch 13. Loss: 0.545677343166, Train_acc 0.801616161616, Val_acc 0.6021\n",
      "Epoch 14. Loss: 0.511070211885, Train_acc 0.800808080808, Val_acc 0.5857\n",
      "Epoch 15. Loss: 0.835425889116, Train_acc 0.596767676768, Val_acc 0.5248\n",
      "Epoch 16. Loss: 0.820438165005, Train_acc 0.734949494949, Val_acc 0.5801\n",
      "Epoch 17. Loss: 0.535792679898, Train_acc 0.742727272727, Val_acc 0.5082\n",
      "Epoch 18. Loss: 0.492582066549, Train_acc 0.75898989899, Val_acc 0.515\n",
      "Epoch 19. Loss: 0.467294593116, Train_acc 0.812626262626, Val_acc 0.5342\n",
      "Epoch 20. Loss: 0.371685802464, Train_acc 0.875353535354, Val_acc 0.5969\n",
      "Epoch 21. Loss: 0.298414905327, Train_acc 0.863838383838, Val_acc 0.6005\n",
      "Epoch 22. Loss: 0.48220542786, Train_acc 0.786565656566, Val_acc 0.5661\n",
      "Epoch 23. Loss: 0.39092037655, Train_acc 0.881717171717, Val_acc 0.5791\n",
      "Epoch 24. Loss: 0.207509554332, Train_acc 0.946868686869, Val_acc 0.6179\n",
      "Epoch 25. Loss: 0.198147776051, Train_acc 0.939696969697, Val_acc 0.5803\n",
      "Epoch 26. Loss: 0.189072274783, Train_acc 0.969292929293, Val_acc 0.6555\n",
      "Epoch 27. Loss: 0.534648055092, Train_acc 0.656363636364, Val_acc 0.4441\n",
      "Epoch 28. Loss: 0.632608720753, Train_acc 0.631919191919, Val_acc 0.4747\n",
      "Epoch 29. Loss: 0.601032548412, Train_acc 0.668686868687, Val_acc 0.4824\n",
      "Epoch 30. Loss: 0.486242554713, Train_acc 0.833737373737, Val_acc 0.5171\n",
      "Epoch 31. Loss: 0.321489138969, Train_acc 0.854848484848, Val_acc 0.4954\n",
      "Epoch 32. Loss: 0.295826493895, Train_acc 0.951515151515, Val_acc 0.5734\n",
      "Epoch 33. Loss: 0.141543110382, Train_acc 0.963838383838, Val_acc 0.5765\n",
      "Epoch 34. Loss: 0.10302552874, Train_acc 0.978080808081, Val_acc 0.5819\n",
      "Epoch 35. Loss: 0.0829825139017, Train_acc 0.978080808081, Val_acc 0.5808\n",
      "Epoch 36. Loss: 0.555858977867, Train_acc 0.460808080808, Val_acc 0.4594\n",
      "Epoch 37. Loss: 1.22680511946, Train_acc 0.557272727273, Val_acc 0.5353\n",
      "Epoch 38. Loss: 0.925293310888, Train_acc 0.659393939394, Val_acc 0.5057\n",
      "Epoch 39. Loss: 0.557633841012, Train_acc 0.888080808081, Val_acc 0.5872\n",
      "Epoch 40. Loss: 0.321453687643, Train_acc 0.892525252525, Val_acc 0.5941\n",
      "Epoch 41. Loss: 0.248392270686, Train_acc 0.886262626263, Val_acc 0.587\n",
      "Epoch 42. Loss: 0.236704650638, Train_acc 0.877676767677, Val_acc 0.5784\n",
      "Epoch 43. Loss: 0.226097767212, Train_acc 0.875050505051, Val_acc 0.5833\n",
      "Epoch 44. Loss: 0.217571699624, Train_acc 0.874141414141, Val_acc 0.5788\n",
      "Epoch 45. Loss: 0.287650596409, Train_acc 0.761919191919, Val_acc 0.497\n",
      "Epoch 46. Loss: 1.14119897723, Train_acc 0.829494949495, Val_acc 0.5644\n",
      "Epoch 47. Loss: 0.414959308569, Train_acc 0.847070707071, Val_acc 0.5354\n",
      "Epoch 48. Loss: 0.377073987518, Train_acc 0.839494949495, Val_acc 0.5216\n",
      "Epoch 49. Loss: 0.391820178094, Train_acc 0.849191919192, Val_acc 0.5338\n",
      "Epoch 50. Loss: 0.421191304451, Train_acc 0.786767676768, Val_acc 0.5314\n",
      "Epoch 51. Loss: 0.481419528103, Train_acc 0.861717171717, Val_acc 0.5808\n",
      "Epoch 52. Loss: 0.360325570869, Train_acc 0.85404040404, Val_acc 0.5479\n",
      "Epoch 53. Loss: 0.349072565968, Train_acc 0.866363636364, Val_acc 0.5567\n",
      "Epoch 54. Loss: 0.347161533168, Train_acc 0.864545454545, Val_acc 0.5492\n",
      "Epoch 55. Loss: 0.43471389884, Train_acc 0.812929292929, Val_acc 0.5503\n",
      "Epoch 56. Loss: 0.348775251636, Train_acc 0.856666666667, Val_acc 0.5715\n",
      "Epoch 57. Loss: 0.256234079973, Train_acc 0.88, Val_acc 0.5512\n",
      "Epoch 58. Loss: 0.208250675649, Train_acc 0.883131313131, Val_acc 0.589\n",
      "Epoch 59. Loss: 0.193407584617, Train_acc 0.891616161616, Val_acc 0.5976\n",
      "Epoch 60. Loss: 0.198725142868, Train_acc 0.924141414141, Val_acc 0.597\n",
      "Epoch 61. Loss: 0.186290037833, Train_acc 0.926868686869, Val_acc 0.5895\n",
      "Epoch 62. Loss: 0.171037906601, Train_acc 0.850808080808, Val_acc 0.5563\n",
      "Epoch 63. Loss: 0.415963263382, Train_acc 0.875252525253, Val_acc 0.5766\n",
      "Epoch 64. Loss: 0.21453877895, Train_acc 0.928787878788, Val_acc 0.5947\n",
      "Epoch 65. Loss: 0.137683727302, Train_acc 0.933333333333, Val_acc 0.5962\n",
      "Epoch 66. Loss: 0.130917713838, Train_acc 0.931212121212, Val_acc 0.5958\n",
      "Epoch 67. Loss: 0.138343138252, Train_acc 0.93404040404, Val_acc 0.5956\n",
      "Epoch 68. Loss: 0.163785045385, Train_acc 0.911111111111, Val_acc 0.5675\n",
      "Epoch 69. Loss: 0.3476966388, Train_acc 0.785252525253, Val_acc 0.5287\n",
      "Epoch 70. Loss: 0.432029469252, Train_acc 0.933535353535, Val_acc 0.6003\n",
      "Epoch 71. Loss: 0.142540332687, Train_acc 0.932727272727, Val_acc 0.5936\n",
      "Epoch 72. Loss: 0.144322997608, Train_acc 0.878181818182, Val_acc 0.5895\n",
      "Epoch 73. Loss: 0.233065220705, Train_acc 0.934545454545, Val_acc 0.5912\n",
      "Epoch 74. Loss: 0.134824784129, Train_acc 0.935656565657, Val_acc 0.5898\n",
      "Epoch 75. Loss: 0.141149861922, Train_acc 0.934747474747, Val_acc 0.5897\n",
      "Epoch 76. Loss: 0.128174780985, Train_acc 0.939696969697, Val_acc 0.5922\n",
      "Epoch 77. Loss: 0.136394604857, Train_acc 0.937171717172, Val_acc 0.5991\n",
      "Epoch 78. Loss: 0.139469089294, Train_acc 0.92797979798, Val_acc 0.5913\n",
      "Epoch 79. Loss: 0.167101092986, Train_acc 0.940101010101, Val_acc 0.5925\n",
      "Epoch 80. Loss: 0.128601397319, Train_acc 0.940909090909, Val_acc 0.5924\n",
      "Epoch 81. Loss: 0.127161092625, Train_acc 0.938282828283, Val_acc 0.5877\n",
      "Epoch 82. Loss: 0.120721982184, Train_acc 0.936464646465, Val_acc 0.594\n",
      "Epoch 83. Loss: 0.115754130434, Train_acc 0.945050505051, Val_acc 0.5915\n",
      "Epoch 84. Loss: 0.184183862397, Train_acc 0.885151515152, Val_acc 0.5923\n",
      "Epoch 85. Loss: 0.371697261674, Train_acc 0.82404040404, Val_acc 0.5655\n",
      "Epoch 86. Loss: 0.780424985821, Train_acc 0.538080808081, Val_acc 0.5312\n",
      "Epoch 87. Loss: 1.11569692809, Train_acc 0.541212121212, Val_acc 0.5249\n",
      "Epoch 88. Loss: 1.09366212908, Train_acc 0.544343434343, Val_acc 0.5254\n",
      "Epoch 89. Loss: 1.09048035356, Train_acc 0.546161616162, Val_acc 0.52\n",
      "Epoch 90. Loss: 1.07068779871, Train_acc 0.576262626263, Val_acc 0.5477\n",
      "Epoch 91. Loss: 1.05691135412, Train_acc 0.577676767677, Val_acc 0.5521\n",
      "Epoch 92. Loss: 1.05325393141, Train_acc 0.577676767677, Val_acc 0.5545\n",
      "Epoch 93. Loss: 1.05257108369, Train_acc 0.578181818182, Val_acc 0.5507\n",
      "Epoch 94. Loss: 1.04285638533, Train_acc 0.575757575758, Val_acc 0.5554\n",
      "Epoch 95. Loss: 1.04390616576, Train_acc 0.564444444444, Val_acc 0.5429\n",
      "Epoch 96. Loss: 0.987268806972, Train_acc 0.591313131313, Val_acc 0.5634\n",
      "Epoch 97. Loss: 0.787683236947, Train_acc 0.702929292929, Val_acc 0.5449\n",
      "Epoch 98. Loss: 0.583461245838, Train_acc 0.81, Val_acc 0.5692\n",
      "Epoch 99. Loss: 0.405241626427, Train_acc 0.897878787879, Val_acc 0.5994\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "moving_loss = 0.\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx).reshape((-1, num_inputs))\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            cross_entropy = loss(output, label)\n",
    "            cross_entropy.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        if i == 0:\n",
    "            moving_loss = nd.mean(cross_entropy).asscalar()\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * nd.mean(cross_entropy).asscalar()\n",
    "\n",
    "    val_accuracy = evaluate_accuracy(val_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Val_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 3.  3.  2.  3.  1.  2.  3.  3.  2.  1.  3.  2.  3.  1.  0.  3.  3.  2.\n",
      "  3.  1.  3.  3.  3.  2.  1.  3.  2.  3.  3.  0.  3.  3.  2.  3.  1.  2.\n",
      "  3.  3.  2.  1.  3.  2.  3.  1.  0.  3.  3.  2.  3.  1.  2.  3.  3.  2.\n",
      "  3.  3.  2.  3.  3.  0.  3.  3.  2.  3.  1.  2.  3.  3.  3.  1.  3.  2.\n",
      "  3.  3.  0.  3.  3.  2.  3.  1.  3.  3.  3.  3.  1.  3.  3.  3.  3.  0.\n",
      "  3.  3.  3.  3.  1.  2.  3.  3.  2.  1.]\n",
      "<NDArray 100 @cpu(0)>\n",
      "['1' '2' 'fizz' '4' 'buzz' 'fizz' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13'\n",
      " 'buzz' 'fizzbuzz' '16' '17' 'fizz' '19' 'buzz' '21' '22' '23' 'fizz'\n",
      " 'buzz' '26' 'fizz' '28' '29' 'fizzbuzz' '31' '32' 'fizz' '34' 'buzz'\n",
      " 'fizz' '37' '38' 'fizz' 'buzz' '41' 'fizz' '43' 'buzz' 'fizzbuzz' '46'\n",
      " '47' 'fizz' '49' 'buzz' 'fizz' '52' '53' 'fizz' '55' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' '61' '62' 'fizz' '64' 'buzz' 'fizz' '67' '68' '69' 'buzz' '71'\n",
      " 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' '81' '82' '83'\n",
      " '84' 'buzz' '86' '87' '88' '89' 'fizzbuzz' '91' '92' '93' '94' 'buzz'\n",
      " 'fizz' '97' '98' 'fizz' 'buzz']\n",
      "0.91\n"
     ]
    }
   ],
   "source": [
    "def fizz_buzz(i, prediction):\n",
    "    if prediction == 0:\n",
    "        return \"fizzbuzz\"\n",
    "    elif prediction == 1:\n",
    "        return \"buzz\"\n",
    "    elif prediction == 2:\n",
    "        return \"fizz\"\n",
    "    else:\n",
    "        return str(i)\n",
    "    \n",
    "test_data.reset()\n",
    "for i, batch in enumerate(test_data):\n",
    "    data = batch.data[0].as_in_context(ctx).reshape((-1, num_inputs))\n",
    "    label = batch.label[0].as_in_context(ctx)\n",
    "    output = net(data)\n",
    "    predictions = nd.argmax(output, axis=1)\n",
    "    print(predictions)\n",
    "    output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "    print(output)\n",
    "    acc = mx.metric.Accuracy()\n",
    "    acc.update(preds=predictions, labels=label)\n",
    "    print(acc.get()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
