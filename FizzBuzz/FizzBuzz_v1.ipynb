{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grokking **FizzBuzz** using `MXNet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last year, Joel Grus wrote a brilliant [article](http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/) on *fizzbuzz*. He had attended an interview and was asked about *fizzbuzz*. He went on to solve it using `tensorflow`. Now, you may wonder why should we use deep learning for this? Atleast, I thought that way and didn't really pay much attention to the code. \n",
    "\n",
    "This summer, I had an opportunity to interview with an AI-startup that I really liked. And guess what? I was asked to solve `fizzbuzz` using deep learning. Long story short, neither Joel nor I got the job ! \n",
    "\n",
    "But this made me think about why `fizzbuzz` makes sense? But before we get on to that, what is this `fizzbuzz` problem?\n",
    "\n",
    "**What is fizzbuzz ?**\n",
    "\n",
    "Given an integer `x`, the output is determined by the following rules:\n",
    "\n",
    "- if `x` is divisible by 3, output is \"fizz\"\n",
    "- if `x` is divisible by 5, output is \"buzz\"\n",
    "- if `x` is divisible by 15, output is \"fizzbuzz\"\n",
    "- else, the output is `x`\n",
    "\n",
    "A typical output sequence will look like this\n",
    "\n",
    "| Input   |      Output      | \n",
    "|----------|:-------------:|\n",
    "| 1 |  1 |\n",
    "| 2 |  2 |\n",
    "| 3 | \"fizz\" |\n",
    "| 4 | 4 |\n",
    "| 5 | \"buzz\" |\n",
    "| 6 | \"fizz\" |\n",
    "| 7 | 7 |\n",
    "| 8 | 8 |\n",
    "| 9 | \"fizz\" |\n",
    "| 10 | \"buzz\" |\n",
    "| 11 | 11 |\n",
    "| 12 | \"fizz\" |\n",
    "| 13 | 13 |\n",
    "| 14 | 14 |\n",
    "| 15 | \"fizzbuzz\" |\n",
    "| 16 | 16 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know the rules that generate the data, there's really no need for machine learning. Unfortunately, in real-life, we only have the data. The goal of machine learning is to learn the function that generated the data. In this aspect, `fizzbuzz` provides us with an easy-to-understand dataset and allows us to understand and explore the algorithms better. \n",
    "\n",
    "What follows below is a pedantic exercise in understanding how `MXNet` can be used to solve the `fizzbuzz` problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is `MXNet`?**\n",
    "\n",
    "`MXNet` is a scalable open-source deep learning framework. It scales to multiple GPUs and multiple machines. At Amazon, `MXNet` is the deep learning framework of choice at AWS. It is supported by Intel, Dato, Baidu, Microsoft, MIT amongst others. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure of the article**\n",
    "\n",
    "In the subsequent sections, we will do the following\n",
    "\n",
    "1. Structure the problem as a multi-class classification problem\n",
    "2. Generate the fizzbuzz data\n",
    "3. Divide the data into train and test\n",
    "4. Build a logistic regression model in `MXNet` from scratch\n",
    "5. Introduce `Gluon`\n",
    "6. Build a multi-layer-perceptron model using `Gluon`\n",
    "7. Build a Convolutional Neural Network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure the problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data, how do we structure this as a machine learning problem? To do supervised machine learning, we need features and a target variable. *Fizzbuzz* can be modeled as a multi-class classification problem. \n",
    "\n",
    "Let's first start with the target. The target can be one of the four classes - \"fizz\", \"buzz\", \"fizzbuzz\" or \"the given number\". The model should predict which of the classes is most likely for the given input number. The 4 classes are encoded and the model will be built. Once the model is built, using the prediction label, we will need a decoder function. The decoder function will convert the label to the corresponding output. \n",
    "\n",
    "Now, let's think of a way to do feature engineering for the input. The input is an integer. One option we could explore is convert the number to its binary representation. The binary representation could be of fixed-length. And each digit of the fixed-length binary representation can be an input feature.\n",
    "\n",
    "An example will explain this better.\n",
    "\n",
    "Let's say, we train the model using the first 1000 integers. To create a binary representation of fixed length, we first need to find the maximum length of the input vector. 2^10 is 1024. So, we need the input vector to be of length 10.\n",
    "\n",
    "The output is encoded as 0, 1, 2, 3 for \"fizzbuzz\", \"buzz\", \"fizz\" and \"the given number\" respectively.\n",
    "\n",
    "**Examples**\n",
    "\n",
    "*Example 1*\n",
    "\n",
    "Input: 100  \n",
    "Output: \"buzz\"  \n",
    "\n",
    "The input feature: 0001100100  \n",
    "Output label: 1  \n",
    "\n",
    "*Example 2*\n",
    "\n",
    "Input: 11  \n",
    "Output: \"the given number\"  \n",
    "\n",
    "The input feature: 0000001011   \n",
    "Output label: 3  \n",
    "\n",
    "Now, let's start building models using `mxnet`. Each digit of the input feature vector will be an input neuron. The output will have 4 neurons, one corresponding to each of the classes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate the data**\n",
    "\n",
    "Let's now generate the data. Let's first import the required libraries `numpy` and `mxnet`.\n",
    "\n",
    "One way to install `mxnet` is to run the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: mxnet in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\n",
      "Requirement already up-to-date: graphviz in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from mxnet)\n",
      "Requirement already up-to-date: numpy in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from mxnet)\n",
      "Requirement already up-to-date: requests in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from mxnet)\n",
      "Requirement already up-to-date: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from requests->mxnet)\n",
      "Requirement already up-to-date: chardet<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from requests->mxnet)\n",
      "Requirement already up-to-date: idna<2.7,>=2.5 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from requests->mxnet)\n",
      "Requirement already up-to-date: urllib3<1.23,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from requests->mxnet)\n"
     ]
    }
   ],
   "source": [
    "#install mxnet. If its already available, upgrade it.\n",
    "!pip install mxnet --upgrade --pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import os\n",
    "mx.random.seed(1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `mxnet`, every array has a context - either on a GPU or CPU. \n",
    "\n",
    "The data size won't be too high for this exercise and CPU should suffice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the context to be CPU\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**helper functions**\n",
    "\n",
    "We need a few helper functions to start with.\n",
    "\n",
    "We need a function to convert the given input integer to its binary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to encode the integer to its binary representation\n",
    "def binary_encode(i, num_digits):\n",
    "    return np.array([i >> d & 1 for d in range(num_digits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function to encode the target into one of the 4 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to encode the target into multi-class\n",
    "def fizz_buzz_encode(i):\n",
    "    if   i % 15 == 0: \n",
    "        return 0\n",
    "    elif i % 5  == 0: \n",
    "        return 1\n",
    "    elif i % 3  == 0: \n",
    "        return 2\n",
    "    else:             \n",
    "        return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is built and we use it to predict it for a given number, we need a function that maps the prediction to the right output. The following function helps us do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given prediction, map it to the correct output label\n",
    "def fizz_buzz(i, prediction):\n",
    "    if prediction == 0:\n",
    "        return \"fizzbuzz\"\n",
    "    elif prediction == 1:\n",
    "        return \"buzz\"\n",
    "    elif prediction == 2:\n",
    "        return \"fizz\"\n",
    "    else:\n",
    "        return str(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create train, validation, test datasets**\n",
    "\n",
    "We now need to create the train, validation and test datasets.\n",
    "\n",
    "Let's generate 100,000 data points. That's the first 100,000 integers.\n",
    "\n",
    "In ideal case, the train, validation and test datasets are randomly selected. But for sake of simplicity, let's use the first 100 integers as test dataset. We won't use this for training. Once we create the model, we will check the accuracy of the model by predicting on the test dataset.\n",
    "\n",
    "We will use the integers 101 to 50,000 as training dataset. We will build the model using this dataset.\n",
    "\n",
    "We will use integers 50,001 to 100,000 as validation dataset. We could use this for hyper-parameter tuning. In this exercise, we wouldn't be doing hyper-parameter tuning though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start by defining the number of integers to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of integers to generate\n",
    "MAX_NUMBER = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the input feature vector has to be determined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The input feature vector is determined by NUM_DIGITS\n",
    "NUM_DIGITS = np.log2(MAX_NUMBER).astype(np.int)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train, test and validation datasets are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate training dataset - both features and labels\n",
    "trainX = np.array([binary_encode(i, NUM_DIGITS) for i in range(101, np.int(MAX_NUMBER/2))])\n",
    "trainY = np.array([fizz_buzz_encode(i)          for i in range(101, np.int(MAX_NUMBER/2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate validation dataset - both features and labels\n",
    "valX = np.array([binary_encode(i, NUM_DIGITS) for i in range(np.int(MAX_NUMBER/2), MAX_NUMBER)])\n",
    "valY = np.array([fizz_buzz_encode(i)          for i in range(np.int(MAX_NUMBER/2), MAX_NUMBER)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate test dataset - both features and labels\n",
    "testX = np.array([binary_encode(i, NUM_DIGITS) for i in range(1, 101)])\n",
    "testY = np.array([fizz_buzz_encode(i)          for i in range(1, 101)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created the train, test and validation datasets, let's use `mxnet's` iterator `NDArrayIter`. It allows us to specify batch size for training and also allows us to set a flag to shuffle the data or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the parameters\n",
    "batch_size = 100\n",
    "num_inputs = NUM_DIGITS\n",
    "num_outputs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create iterator for train, test and validation datasets\n",
    "train_data = mx.io.NDArrayIter(trainX, trainY,\n",
    "                               batch_size, shuffle=True)\n",
    "val_data = mx.io.NDArrayIter(valX, valY,\n",
    "                               batch_size, shuffle=True)\n",
    "test_data = mx.io.NDArrayIter(testX, testY,\n",
    "                              batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to write another helper function. This one is to evaluate the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to evaluate accuracy of the model\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    data_iterator.reset()\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return predictions,acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression from scratch**\n",
    "\n",
    "Now that we have the data and the helper functions in place, let's go ahead and build logistic regression from scratch. [This](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter02_supervised-learning/softmax-regression-scratch.ipynb) article does a wonderful job of building a multi-class logistic regression from scratch using `mxnet`. We will follow a similar template for our problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mxnet` comes with a `autograd` package. `autograd` enables automatic differentiation of NDArray operations and is used to compute the gradients of the loss function with respect to the model weights. More about `autograd` can be found [here](https://mxnet.incubator.apache.org/api/python/autograd.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import autograd package\n",
    "from mxnet import autograd, nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to initialize the bias and weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the weight and bias matrix\n",
    "\n",
    "#weights matrix\n",
    "W = nd.random_normal(shape=(num_inputs, num_outputs))\n",
    "#bias matrix\n",
    "b = nd.random_normal(shape=num_outputs)\n",
    "\n",
    "#Model parameters\n",
    "params = [W, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to allocate space for each parameter's gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the output to be the probability for each of the classes. The sum of the probabilities should sum upto one. This is accomplished by the softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y_linear):\n",
    "    exp = nd.exp(y_linear-nd.max(y_linear))\n",
    "    norms = nd.sum(exp, axis=0, exclude=True).reshape((-1,1))\n",
    "    return exp / norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function we will be using is softmax cross entropy. Cross-entropy maximizes the log-likelihood given to the correct labels. More about softmax cross-entropy can be read [here](https://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "def softmax_cross_entropy(yhat, y):\n",
    "    return - nd.nansum(y * nd.log(yhat), axis=0, exclude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    y_linear = nd.dot(X, W) + b\n",
    "    yhat = softmax(y_linear)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model to learn(update) the model parameters(weights and biases), we need to define an optimizer. Stochastic Gradient Descent is one of the most popular methods to update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the optimizer\n",
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets execute the training loops. For each training step, batch_size determines the number of data points that will be passed through the network to learn. Once all data points are passed through the network, one epoch is completed. The parameter `epochs` define the number of times the entire dataset has to be cycled through the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper paramters for the training\n",
    "epochs = 50\n",
    "learning_rate = .01\n",
    "best_accuracy = 0.\n",
    "smoothing_constant = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now execute the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 1.19895967839, Train_acc 0.390881763527, Val_acc 0.32814\n",
      "Epoch 1. Loss: 1.18856283526, Train_acc 0.390941883768, Val_acc 0.32654\n",
      "Epoch 2. Loss: 1.18846328616, Train_acc 0.390921843687, Val_acc 0.32636\n",
      "Epoch 3. Loss: 1.18845823074, Train_acc 0.390921843687, Val_acc 0.3263\n",
      "Epoch 4. Loss: 1.18845749429, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 5. Loss: 1.18845736231, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 6. Loss: 1.18845734851, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 7. Loss: 1.18845734258, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 8. Loss: 1.1884573416, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 9. Loss: 1.18845734008, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 10. Loss: 1.18845734178, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 11. Loss: 1.18845734476, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 12. Loss: 1.18845734533, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 13. Loss: 1.1884573445, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 14. Loss: 1.18845734339, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 15. Loss: 1.18845734593, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 16. Loss: 1.18845734451, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 17. Loss: 1.18845734167, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 18. Loss: 1.18845733912, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 19. Loss: 1.18845734221, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 20. Loss: 1.18845734598, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 21. Loss: 1.1884573443, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 22. Loss: 1.18845734144, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 23. Loss: 1.18845734198, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 24. Loss: 1.18845734148, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 25. Loss: 1.18845734184, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 26. Loss: 1.18845734045, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 27. Loss: 1.18845733968, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 28. Loss: 1.18845734183, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 29. Loss: 1.18845734148, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 30. Loss: 1.18845734462, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 31. Loss: 1.18845734927, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 32. Loss: 1.18845734063, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 33. Loss: 1.18845733826, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 34. Loss: 1.18845734258, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 35. Loss: 1.1884573429, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 36. Loss: 1.18845734566, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 37. Loss: 1.18845734795, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 38. Loss: 1.18845734582, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 39. Loss: 1.18845734286, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 40. Loss: 1.18845734184, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 41. Loss: 1.18845734097, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 42. Loss: 1.18845734238, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 43. Loss: 1.18845734224, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 44. Loss: 1.18845734374, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 45. Loss: 1.18845734747, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 46. Loss: 1.18845734205, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 47. Loss: 1.18845734239, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 48. Loss: 1.18845734556, Train_acc 0.390921843687, Val_acc 0.32632\n",
      "Epoch 49. Loss: 1.18845733944, Train_acc 0.390921843687, Val_acc 0.32632\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    #at the start of each epoch, the train data iterator is reset\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 4)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0)) \n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "    #the training and validation accuracies are computed\n",
    "    _,val_accuracy = evaluate_accuracy(val_data, net)\n",
    "    _,train_accuracy = evaluate_accuracy(train_data, net)\n",
    "            \n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Val_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 100 epochs, the train accuracy and validation accuracy doesn't seem to be great. It is only around 53%. Let's see what's the accuracy on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' 'fizz' '15' '16'\n",
      " '17' '18' '19' '20' '21' '22' '23' '24' '25' 'fizz' '27' '28' '29' 'fizz'\n",
      " '31' '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43' '44' '45'\n",
      " '46' '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57' '58' '59' '60'\n",
      " '61' '62' '63' '64' '65' '66' '67' '68' '69' '70' '71' '72' '73' 'fizz'\n",
      " '75' '76' '77' 'fizz' '79' '80' '81' '82' '83' '84' '85' '86' '87' '88'\n",
      " '89' 'fizz' '91' '92' '93' 'fizz' '95' '96' '97' '98' '99' '100']\n",
      "Test Accuracy :  0.5\n"
     ]
    }
   ],
   "source": [
    "#model accuracy on the test dataset\n",
    "predictions,test_accuracy = evaluate_accuracy(test_data, net)\n",
    "output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "print(output)\n",
    "print(\"Test Accuracy : \",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the test dataset is also 53%. The model doesn't seem to be working well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look into doing the same using `Gluon`\n",
    "\n",
    "**What is `Gluon`?**\n",
    "\n",
    "Gluon package is a high-level interface for MXNet. Gluon supports both imperative and symbolic programming, making it easy to train complex models imperatively. More details can be found [here](https://mxnet.incubator.apache.org/api/python/gluon.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-layer perceptron using `Gluon`**\n",
    "\n",
    "Let's build a multi-layer perceptron(MLP) model using `gluon`. \n",
    "\n",
    "MLP is one of the simplest deep learning models. More details can be found on [wikipedia](https://en.wikipedia.org/wiki/Multilayer_perceptron)\n",
    "![](img/MLP.jpg)\n",
    "\n",
    "[image source](http://file.scirp.org/Html/2-2200704_39386.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to import `gluon`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gluon\n",
    "from mxnet import gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the training, test and validation iterators\n",
    "train_data.reset()\n",
    "val_data.reset()\n",
    "test_data.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define the Gluon Sequential Model. Each hidden layer is added sequentially. the `num_hidden` variable defines the number of neurons in each of the hidden layers. The `relu` activation function is used in each of the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define number of neurons in each hidden layer\n",
    "num_hidden = 60\n",
    "#Define the sequential network\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(num_inputs, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous exercise, where we built the model from scratch, the parameters need to be initialized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize parameters\n",
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is defined to be softmax cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the loss function\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer is chosen to be stochastic gradient descent, similar to the previous model. For this model, we define both the learning rate and momentum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the optimizer\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .02,'momentum':0.9})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define variables/hyper-paramters\n",
    "epochs = 50\n",
    "moving_loss = 0.\n",
    "best_accuracy = 0.\n",
    "best_epoch = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 0. Loss: 1.14180620348, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 1. Loss: 1.13989462578, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 2. Loss: 1.13963253198, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 3. Loss: 1.13949417489, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 4. Loss: 1.13932395943, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 5. Loss: 1.13911541501, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 6. Loss: 1.13891853278, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 7. Loss: 1.13875388138, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 8. Loss: 1.13859026167, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 9. Loss: 1.13837527457, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 10. Loss: 1.13806588195, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 11. Loss: 1.13752388365, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 12. Loss: 1.13633316881, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 13. Loss: 1.13367037083, Train_acc 0.533346693387, Val_acc 0.53332\n",
      "Epoch 14. Loss: 1.12727259558, Train_acc 0.533346693387, Val_acc 0.53316\n",
      "Epoch 15. Loss: 1.1159466326, Train_acc 0.533206412826, Val_acc 0.53228\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 16. Loss: 1.08630142366, Train_acc 0.555791583166, Val_acc 0.5372\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 17. Loss: 0.996143116272, Train_acc 0.598316633267, Val_acc 0.54678\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 18. Loss: 0.886869308832, Train_acc 0.65627254509, Val_acc 0.58498\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 19. Loss: 0.700148756826, Train_acc 0.713667334669, Val_acc 0.64526\n",
      "Epoch 20. Loss: 0.588981354647, Train_acc 0.729559118236, Val_acc 0.61648\n",
      "deleting previous checkpoint...\n",
      "Best validation accuracy found. Checkpointing...\n",
      "Epoch 21. Loss: 0.522851111777, Train_acc 0.757635270541, Val_acc 0.66982\n",
      "Epoch 22. Loss: 0.354446786502, Train_acc 0.876673346693, Val_acc 0.64606\n",
      "Epoch 23. Loss: 0.229962290383, Train_acc 0.913446893788, Val_acc 0.57052\n",
      "Epoch 24. Loss: 0.156772776806, Train_acc 0.939699398798, Val_acc 0.59422\n",
      "Epoch 25. Loss: 0.121912966726, Train_acc 0.942284569138, Val_acc 0.58118\n",
      "Epoch 26. Loss: 0.108443143268, Train_acc 0.962825651303, Val_acc 0.59196\n",
      "Epoch 27. Loss: 0.0934781572937, Train_acc 0.972204408818, Val_acc 0.59816\n",
      "Epoch 28. Loss: 0.311564842912, Train_acc 0.939679358717, Val_acc 0.51088\n",
      "Epoch 29. Loss: 0.147195957257, Train_acc 0.948336673347, Val_acc 0.49814\n",
      "Epoch 30. Loss: 0.128911848118, Train_acc 0.950561122244, Val_acc 0.51074\n",
      "Epoch 31. Loss: 0.114654408381, Train_acc 0.950601202405, Val_acc 0.5343\n",
      "Epoch 32. Loss: 0.100298359796, Train_acc 0.95498997996, Val_acc 0.50398\n",
      "Epoch 33. Loss: 0.0886765482039, Train_acc 0.95997995992, Val_acc 0.49584\n",
      "Epoch 34. Loss: 0.0817575939552, Train_acc 0.967214428858, Val_acc 0.4927\n",
      "Epoch 35. Loss: 0.0770022799513, Train_acc 0.976392785571, Val_acc 0.50778\n",
      "Epoch 36. Loss: 0.0736746154507, Train_acc 0.975731462926, Val_acc 0.49308\n",
      "Epoch 37. Loss: 0.0722125155137, Train_acc 0.975330661323, Val_acc 0.48998\n",
      "Epoch 38. Loss: 0.0660630482681, Train_acc 0.986613226453, Val_acc 0.50052\n",
      "Epoch 39. Loss: 0.0671881622572, Train_acc 0.986392785571, Val_acc 0.503\n",
      "Epoch 40. Loss: 0.0726517785585, Train_acc 0.991923847695, Val_acc 0.52164\n",
      "Epoch 41. Loss: 0.810793314983, Train_acc 0.532745490982, Val_acc 0.53066\n",
      "Epoch 42. Loss: 0.991344336697, Train_acc 0.649498997996, Val_acc 0.55384\n",
      "Epoch 43. Loss: 0.737528341414, Train_acc 0.777595190381, Val_acc 0.44388\n",
      "Epoch 44. Loss: 0.59438050748, Train_acc 0.766513026052, Val_acc 0.4435\n",
      "Epoch 45. Loss: 0.502606669253, Train_acc 0.825350701403, Val_acc 0.46366\n",
      "Epoch 46. Loss: 0.328278567287, Train_acc 0.870501002004, Val_acc 0.49886\n",
      "Epoch 47. Loss: 0.288036990916, Train_acc 0.885470941884, Val_acc 0.51054\n",
      "Epoch 48. Loss: 0.248849250659, Train_acc 0.937615230461, Val_acc 0.52506\n",
      "Epoch 49. Loss: 0.187508000142, Train_acc 0.948657314629, Val_acc 0.5302\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "for e in range(epochs):\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            cross_entropy = loss(output, label)\n",
    "            cross_entropy.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        if i == 0:\n",
    "            moving_loss = nd.mean(cross_entropy).asscalar()\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * nd.mean(cross_entropy).asscalar()\n",
    "\n",
    "    _,val_accuracy = evaluate_accuracy(val_data, net)\n",
    "    _,train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    \n",
    "    if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            if best_epoch!=-1:\n",
    "                print('deleting previous checkpoint...')\n",
    "                os.remove('mlp-%d.params'%(best_epoch))\n",
    "            best_epoch = e\n",
    "            print('Best validation accuracy found. Checkpointing...')\n",
    "            net.save_params('mlp-%d.params'%(e))\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Val_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy looks good - but validation accuracy is terrible. This generally is a sign of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now predict it on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the parameters\n",
    "net.load_params('mlp-%d.params'%(best_epoch), ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' 'fizz' '4' 'buzz' '6' '7' '8' '9' 'buzz' '11' 'fizz' '13' '14'\n",
      " 'fizzbuzz' '16' '17' 'fizz' '19' 'buzz' 'fizz' '22' '23' 'fizz' 'buzz'\n",
      " '26' 'fizz' '28' '29' 'buzz' '31' '32' 'fizz' '34' 'buzz' '36' '37' '38'\n",
      " '39' 'buzz' '41' '42' '43' '44' 'fizzbuzz' '46' '47' 'fizz' '49' 'buzz'\n",
      " '51' '52' '53' '54' 'buzz' '56' 'fizz' '58' '59' 'buzz' '61' '62' '63'\n",
      " '64' 'buzz' 'fizz' '67' '68' 'fizz' 'buzz' '71' 'fizz' '73' '74'\n",
      " 'fizzbuzz' '76' '77' '78' '79' 'buzz' 'fizz' '82' '83' 'fizz' 'buzz' '86'\n",
      " '87' '88' '89' 'buzz' '91' '92' 'fizz' '94' 'buzz' 'fizz' '97' '98' '99'\n",
      " 'buzz']\n",
      "Test Accuracy :  0.86\n"
     ]
    }
   ],
   "source": [
    "#predict on the test dataset\n",
    "predictions,test_accuracy = evaluate_accuracy(test_data, net)\n",
    "output = np.vectorize(fizz_buzz)(np.arange(1, 101), predictions.asnumpy().astype(np.int))\n",
    "print(output)\n",
    "print(\"Test Accuracy : \",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy is 83%. Let's try to explore another model architecture to see how the model accuracy is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion and Next Steps**\n",
    "\n",
    "This article gave a quick overview of how to get started with `mxnet` and `gluon`. With `gluon`, prototyping and experimenting with various architectures is much faster. `autograd` allows us to record computation history on the fly so as to calculate gradients later. \n",
    "\n",
    "[The straight dope](https://github.com/zackchase/mxnet-the-straight-dope/) has a good set of tutorials to get started with `mxnet`.\n",
    "\n",
    "While this article briefly touched upon Convolutional Neural Networks, it didn't provide enough motivation and real-life use cases. While we saw models overfitting, we didn't get into how to overcome overfitting. In the next article, we will show how to build models to do image recognition with some real-life use cases.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
